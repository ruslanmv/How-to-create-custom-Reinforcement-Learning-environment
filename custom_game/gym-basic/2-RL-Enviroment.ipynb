{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1363d0b8",
   "metadata": {},
   "source": [
    "# 3 - RL Enviroment \n",
    "In this part we are going to build the most essential Enviroment to create a RL  Pipeline.\n",
    "\n",
    "The first framework that we are going to use is the  **RAY**\n",
    "\n",
    "We are going to  pass either a string name or a Python class to specify an environment.  In particular we are going to choose the simplest local enviroment.\n",
    "\n",
    "Custom env classes passed directly to the algorithm must take a single env_config parameter in their constructor:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e085e",
   "metadata": {},
   "source": [
    "### Example 1 - Gym + Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32caadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "from gym import spaces\n",
    "from ray.rllib.algorithms import ppo\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "       # There are two actions, first will get reward of 1, second reward of -1. \n",
    "        self.action_space = spaces.Discrete(5)      #<gym.Space>\n",
    "        self.observation_space = spaces.Discrete(2) #<gym.Space>\n",
    "    \n",
    "    def reset(self):\n",
    "        state = 0\n",
    "        #return <obs>\n",
    "        return state\n",
    "                           \n",
    "    def step(self, action):\n",
    "\n",
    "        # if we took an action, we were in state 1\n",
    "        state = 1\n",
    "    \n",
    "        if action == 2:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        # regardless of the action, game is done after a single step\n",
    "        done = True\n",
    "\n",
    "        info = {}\n",
    "        # return <obs>, <reward: float>, <done: bool>, <info: dict>\n",
    "        return state, reward, done, info   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b96cc",
   "metadata": {},
   "source": [
    "Python 3.8.x\n",
    "ray 1.0\n",
    "tensorflow 2.3.1\n",
    "tensorflow-probability 0.11\n",
    "gym 0.17.3\n",
    "pygame 2.0.0\n",
    "\n",
    "numpy==1.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8be9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "62ec2e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 22:55:17,824\tINFO worker.py:1538 -- Started a local Ray instance.\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "Could not import PyTorch! RLlib requires you to install at least one deep-learning framework: `pip install [torch|tensorflow|jax]`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\utils\\framework.py:187\u001b[0m, in \u001b[0;36mtry_import_torch\u001b[1;34m(error)\u001b[0m\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 187\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnn\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtune\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSAC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# reinforced learning agent\u001b[39;49;00m\n\u001b[0;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTraining1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_freq\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcheckpoint_at_end\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlocal_dir\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m./ray_results/\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mMyEnv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_workers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mnum_cpus_per_worker\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43menv_config\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_steps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexport_frames\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexport_states\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# \"reward_mode\": \"continuous\",\u001b[39;49;00m\n\u001b[0;32m     16\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# \"env_flipped\": True,\u001b[39;49;00m\n\u001b[0;32m     17\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;66;43;03m# \"env_flipmode\": True,\u001b[39;49;00m\n\u001b[0;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\n\u001b[0;32m     21\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtimesteps_total\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5_000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\tune\\tune.py:484\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(run_or_experiment, name, metric, mode, stop, time_budget_s, config, resources_per_trial, num_samples, local_dir, search_alg, scheduler, keep_checkpoints_num, checkpoint_score_attr, checkpoint_freq, checkpoint_at_end, verbose, progress_reporter, log_to_file, trial_name_creator, trial_dirname_creator, chdir_to_trial_dir, sync_config, export_formats, max_failures, fail_fast, restore, server_port, resume, reuse_actors, trial_executor, raise_on_failed_trial, callbacks, max_concurrent_trials, _experiment_checkpoint_dir, _remote, _remote_string_queue)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reuse_actors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    477\u001b[0m     trainable \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    478\u001b[0m         run_or_experiment\u001b[38;5;241m.\u001b[39mrun_identifier\n\u001b[0;32m    479\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(run_or_experiment, Experiment)\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m run_or_experiment\n\u001b[0;32m    481\u001b[0m     )\n\u001b[0;32m    482\u001b[0m     reuse_actors \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    483\u001b[0m         \u001b[38;5;66;03m# Only default to True for function trainables that meet certain conditions\u001b[39;00m\n\u001b[1;32m--> 484\u001b[0m         \u001b[43mis_function_trainable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    485\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m    486\u001b[0m             \u001b[38;5;66;03m# Changing resources requires restarting actors\u001b[39;00m\n\u001b[0;32m    487\u001b[0m             scheduler\n\u001b[0;32m    488\u001b[0m             \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(scheduler, ResourceChangingScheduler)\n\u001b[0;32m    489\u001b[0m         )\n\u001b[0;32m    490\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m    491\u001b[0m             \u001b[38;5;66;03m# If GPUs are requested we could run into problems with device memory\u001b[39;00m\n\u001b[0;32m    492\u001b[0m             _check_gpus_in_resources(resources_per_trial)\n\u001b[0;32m    493\u001b[0m         )\n\u001b[0;32m    494\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m    495\u001b[0m             \u001b[38;5;66;03m# If the resource request is overridden, we don't know if GPUs\u001b[39;00m\n\u001b[0;32m    496\u001b[0m             \u001b[38;5;66;03m# will be requested, yet, so default to False\u001b[39;00m\n\u001b[0;32m    497\u001b[0m             _check_default_resources_override(trainable)\n\u001b[0;32m    498\u001b[0m         )\n\u001b[0;32m    499\u001b[0m     )\n\u001b[0;32m    501\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    502\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(scheduler, (PopulationBasedTraining, PopulationBasedTrainingReplay))\n\u001b[0;32m    503\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m reuse_actors\n\u001b[0;32m    504\u001b[0m ):\n\u001b[0;32m    505\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    506\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConsider boosting PBT performance by enabling `reuse_actors` as \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    507\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwell as implementing `reset_config` for Trainable.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    508\u001b[0m     )\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\tune\\registry.py:64\u001b[0m, in \u001b[0;36mis_function_trainable\u001b[1;34m(trainable)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Check if a given trainable is a function trainable.\"\"\"\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trainable, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m---> 64\u001b[0m     trainable \u001b[38;5;241m=\u001b[39m \u001b[43mget_trainable_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trainable, \u001b[38;5;28mtype\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28misinstance\u001b[39m(trainable, FunctionType)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(trainable, partial)\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;129;01mor\u001b[39;00m callable(trainable)\n\u001b[0;32m     70\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\tune\\registry.py:45\u001b[0m, in \u001b[0;36mget_trainable_cls\u001b[1;34m(trainable_name)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;129m@DeveloperAPI\u001b[39m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_trainable_cls\u001b[39m(trainable_name):\n\u001b[1;32m---> 45\u001b[0m     \u001b[43mvalidate_trainable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainable_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _global_registry\u001b[38;5;241m.\u001b[39mget(TRAINABLE_CLASS, trainable_name)\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\tune\\registry.py:55\u001b[0m, in \u001b[0;36mvalidate_trainable\u001b[1;34m(trainable_name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_trainable(trainable_name):\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;66;03m# Make sure everything rllib-related is registered.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _register_all\n\u001b[1;32m---> 55\u001b[0m     \u001b[43m_register_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _has_trainable(trainable_name):\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m TuneError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnknown trainable: \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m trainable_name)\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\__init__.py:39\u001b[0m, in \u001b[0;36m_register_all\u001b[1;34m()\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontrib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregistry\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CONTRIBUTED_ALGORITHMS\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, get_trainable_class_and_config \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(ALGORITHMS\u001b[38;5;241m.\u001b[39mitems()) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m     37\u001b[0m     CONTRIBUTED_ALGORITHMS\u001b[38;5;241m.\u001b[39mitems()\n\u001b[0;32m     38\u001b[0m ):\n\u001b[1;32m---> 39\u001b[0m     register_trainable(key, \u001b[43mget_trainable_class_and_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__fake\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sigmoid_fake_data\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__parameter_tuning\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m     42\u001b[0m     register_trainable(key, _get_algorithm_class(key))\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\algorithms\\registry.py:173\u001b[0m, in \u001b[0;36m_import_qmix\u001b[1;34m()\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_import_qmix\u001b[39m():\n\u001b[1;32m--> 173\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqmix\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mqmix\u001b[39;00m\n\u001b[0;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m qmix\u001b[38;5;241m.\u001b[39mQMix, qmix\u001b[38;5;241m.\u001b[39mQMixConfig()\u001b[38;5;241m.\u001b[39mto_dict()\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqmix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqmix\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QMix, QMixConfig, DEFAULT_CONFIG\n\u001b[0;32m      3\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQMix\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQMixConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDEFAULT_CONFIG\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithm_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AlgorithmConfig, NotProvided\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimple_q\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimple_q\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleQ, SimpleQConfig\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqmix\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mqmix_policy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QMixTorchPolicy\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreplay_buffers\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m update_priorities_in_replay_buffer\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexecution\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrollout_ops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      8\u001b[0m     synchronous_parallel_sample,\n\u001b[0;32m      9\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\algorithms\\qmix\\qmix_policy.py:25\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_utils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m apply_grad_clipping\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Torch must be installed.\u001b[39;00m\n\u001b[1;32m---> 25\u001b[0m torch, nn \u001b[38;5;241m=\u001b[39m \u001b[43mtry_import_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43merror\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m logger \u001b[38;5;241m=\u001b[39m logging\u001b[38;5;241m.\u001b[39mgetLogger(\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mQMixLoss\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\utils\\framework.py:193\u001b[0m, in \u001b[0;36mtry_import_torch\u001b[1;34m(error)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m error:\n\u001b[1;32m--> 193\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m    194\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not import PyTorch! RLlib requires you to \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    195\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minstall at least one deep-learning framework: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    196\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install [torch|tensorflow|jax]`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    197\u001b[0m         )\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _torch_stubs()\n",
      "\u001b[1;31mImportError\u001b[0m: Could not import PyTorch! RLlib requires you to install at least one deep-learning framework: `pip install [torch|tensorflow|jax]`."
     ]
    }
   ],
   "source": [
    "tune.run(\n",
    "    \"SAC\", # reinforced learning agent\n",
    "    name = \"Training1\",\n",
    "    checkpoint_freq = 100,\n",
    "    checkpoint_at_end = True,\n",
    "    local_dir = r'./ray_results/',\n",
    "    config={\n",
    "        \"env\": MyEnv,\n",
    "        \"num_workers\": 30,\n",
    "        \"num_cpus_per_worker\": 0.5,\n",
    "        \"env_config\":{\n",
    "            \"max_steps\": 100,\n",
    "            \"export_frames\": False,\n",
    "            \"export_states\": False,\n",
    "            # \"reward_mode\": \"continuous\",\n",
    "            # \"env_flipped\": True,\n",
    "            # \"env_flipmode\": True,\n",
    "            }\n",
    "        },\n",
    "    stop = {\n",
    "        \"timesteps_total\": 5_000,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()\n",
    "#algo = ppo.PPO(env=MyEnv, config={\"env_config\": {},  # config to pass to env class\n",
    "#})\n",
    "\n",
    "#algo = ppo.PPO(env=MyEnv, config=config) \n",
    "\n",
    "algo = ppo.PPO(env=MyEnv,config={\"num_workers\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae30180",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ppo = []\n",
    "for _ in range(25):\n",
    "    result = algo.train()\n",
    "    print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "    mean_ppo.append(result['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1bafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = [x for x in range(len(mean_ppo))]\n",
    "\n",
    "plt.plot(xs, mean_ppo)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63087033",
   "metadata": {},
   "source": [
    "### How to use the trained algorithm in RL with PP0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1565fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa82a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc021f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Fix the windows path\n",
    "#evaluation = trainer.evaluate(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a91c8b",
   "metadata": {},
   "source": [
    "## Computing actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d32737",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1207b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations) # The state which you should determine the action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0f9a8",
   "metadata": {},
   "source": [
    "Given any state compute the action which you get the maximum reward in according to the traning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8014b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = trainer.compute_single_action(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1bade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not done:\n",
    "    action = trainer.compute_single_action(observations)\n",
    "    observations, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(\"observations, reward, done, info\",observations, reward, done, info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805eca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = trainer.compute_actions({\"obs_1\": observations, \"obs_2\": observations})\n",
    "print(action)\n",
    "# {'obs_1': 0, 'ob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630100cd",
   "metadata": {},
   "source": [
    "In the following rl test we are going to use  stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ffc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pygame\n",
    "from pygame.surfarray import array3d\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = MyEnv()\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5eb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = f\"models/{int(time.time())}/\"\n",
    "logdir = f\"logs/{int(time.time())}/\"\n",
    "\n",
    "fps_controller = pygame.time.Clock()\n",
    "fps_controller.tick(60)\n",
    "\n",
    "# Checks for errors encountered\n",
    "pygame.init()\n",
    "\n",
    "# Initialise game window\n",
    "pygame.display.set_caption('Training')\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "\n",
    "env = DummyVecEnv([lambda: Monitor(MyEnv(),logdir,allow_early_resets=True)])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env,verbose=1, tensorboard_log=logdir,n_epochs=10)\n",
    "\n",
    "TIMESTEPS = 100\n",
    "iters = 0\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a5522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "473615a2",
   "metadata": {},
   "source": [
    "In the following example we are interested to include pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93878ceb",
   "metadata": {},
   "source": [
    "### Example 2 - Gym + Ray + Pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75cc0f",
   "metadata": {},
   "source": [
    "In order to create an appropiate gym enviroment applied to ray and pygame we need need to pay attention into two gym objects:\n",
    "\n",
    "-- action (object): The action to be performed in the step() function. \n",
    "In a game of chess, the action would be the specific, legal move performed by a player.\n",
    "\n",
    "--observation (object): This is all the information available to the agent to choose the next action. \n",
    "The observation is based only on the current state of the environment.\n",
    "\n",
    "## Observation_space and Action_space\n",
    "In  particular  the observation_space and action_space:\n",
    "there are only certain actions and observations are valid in regards of a specific environment. \n",
    "\n",
    "To define a format, the observation_space and action_space variables need to be assigned to a respective gym.space class.\n",
    "\n",
    "Spaces can differ in their dimensionality and their value range. Continuous and discrete spaces are both possible.\n",
    "\n",
    "self.observation_space = <gym.space>\n",
    "self.action_space = <gym.space>\n",
    "\n",
    "\n",
    "We are going to consider an enviroment where there are two\n",
    "points, one red and one blu. The purpose of the game is give a blue point ( worker) where intercept the red point (target)\n",
    "\n",
    "## Definition of action space\n",
    "\n",
    "We want to control the position of the blue point.\n",
    "\n",
    "So the action is the position, the action are the coordinates that you provides to the enviroment\n",
    "\n",
    "action =[x, y]\n",
    "\n",
    "The value of each coordinate are continous and must be in the range of the size of the horizontal box\n",
    "\n",
    "gym.spaces.Box(low=min_x., high=max_x., shape=(2,), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a87e7",
   "metadata": {},
   "source": [
    "# Analysis of Spaces\n",
    "Before we continue le us check some examples of spaces in order to understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4eb6f",
   "metadata": {},
   "source": [
    "## Box\n",
    "\n",
    "Box - Supports continuous (and discrete) vectors or matrices, used for vector observations, images, etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791065db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box, Discrete,MultiBinary , MultiDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "observation_space = Box(low=-1.0, high=2.0, shape=(3, 4), dtype=np.float32)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301722b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2\n",
    "observation_space = Box(low=np.array([-1.0, -2.0]), high=np.array([2.0, 4.0]), dtype=np.float32)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2\n",
    "observation_space = Box(low=np.array(-1.0), high=np.array(2.0), dtype=np.float32)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c906d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "observation_space = Box(low=0, high=200, shape=(2,), dtype=np.float32)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240269e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(observation_space.sample()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491b366",
   "metadata": {},
   "source": [
    "## Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceba308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 3\n",
    "observation_space =Discrete(2)            # {0, 1}\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 4\n",
    "observation_space =Discrete(3)  # {0, 1, 2}\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a723878f",
   "metadata": {},
   "source": [
    "## MultiBinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04342033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5\n",
    "observation_space = MultiBinary(5)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5\n",
    "observation_space = MultiBinary(2)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e85da7",
   "metadata": {},
   "source": [
    "# MultiDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6\n",
    "observation_space =  MultiDiscrete(np.array([[1, 2], [3, 4]]))\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade45ca",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6a\n",
    "#observation_space =Text(5)\n",
    "# {\"0\", \"42\", \"0123456789\", ...}\n",
    "#observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6b\n",
    "#import string\n",
    "#observation_space = Text(min_length = 1,\n",
    "#     max_length = 10,\n",
    "#     charset = string.digits)\n",
    "#observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe9905",
   "metadata": {},
   "source": [
    "# Dict\n",
    "Elements of this space are (ordered) dictionaries of elements from the constituent spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7\n",
    "from gym.spaces import Dict, Discrete\n",
    "observation_space = Dict({\"position\": Discrete(2), \"velocity\": Discrete(3)})\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 8 [nested]:\n",
    "from gym.spaces import Box, Dict, Discrete, MultiBinary, MultiDiscrete\n",
    "observation_space =Dict(\n",
    "    {\n",
    "        \"ext_controller\": MultiDiscrete([5, 2, 2]),\n",
    "        \"inner_state\": Dict(\n",
    "            {\n",
    "                \"charge\": Discrete(100),\n",
    "                \"system_checks\": MultiBinary(10),\n",
    "                \"job_status\": Dict(\n",
    "                    {\n",
    "                        \"task\": Discrete(5),\n",
    "                        \"progress\": Box(low=0, high=100, shape=()),\n",
    "                    }\n",
    "                ),\n",
    "            }\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 9\n",
    "from gym.spaces import Box, Discrete\n",
    "observation_space = Dict({\"position\": Box(-1, 1, shape=(2,)), \"color\": Discrete(3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07829e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('seats_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3181278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#id  x, y z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba961110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle(\"./employees.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791152fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00191e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 10\n",
    "observation_space = = gym.spaces.Dict(\n",
    "    {\"x_position\": gym.spaces.Box(low=0, high=6, shape=(self.max_sit,), dtype=np.uint8),\n",
    "     \"y_position\": gym.spaces.Box(low=0, high=6, shape=(self.max_sit,), dtype=np.uint8),\n",
    "     \"cluster\": gym.spaces.Box(low=0, high=6, shape=(self.max_sit,), dtype=np.uint8),\n",
    "     \"project\": gym.spaces.Box(low=0, high=6, shape=(self.max_sit,), dtype=np.uint8),\n",
    "     \"energy_consumption\": gym.spaces.Box(low=0, high=1, shape=(self.max_sit,)),\n",
    "     \"emp_project\": gym.spaces.Box(low=0, high=6, shape=(1,), dtype=np.int32),\n",
    "     \"emp_energy_consumption\": gym.spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b425077",
   "metadata": {},
   "source": [
    "# Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d539ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 10\n",
    "from gym.spaces import Box, Discrete, Tuple\n",
    "observation_space = Tuple((Discrete(2), Box(-1, 1, shape=(2,))))\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe075c02",
   "metadata": {},
   "source": [
    "# Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc90c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 11\n",
    "#from gym.spaces import Sequence\n",
    "#space = Sequence(Box(0, 1))\n",
    "#space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71d349",
   "metadata": {},
   "source": [
    "for futher information visit\n",
    "https://gymnasium.farama.org/api/spaces/composite/#gymnasium.spaces.Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb061b",
   "metadata": {},
   "source": [
    "# ---- Summary---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4396bf",
   "metadata": {},
   "source": [
    "Discrete - Supports a single discrete number of values with an optional start for the values\n",
    "\n",
    "MultiDiscrete - Supports single or matrices of binary values, used for holding down a button or if an agent has an object\n",
    "\n",
    "MultiBinary - Supports multiple discrete values with multiple axes, used for controller actions\n",
    "\n",
    "Text - Supports strings, used for passing agent messages, mission details, etc\n",
    "\n",
    "Composite Spaces\n",
    "Often environment spaces require joining fundamental spaces together for vectorised environments, separate agents or readability of the space.\n",
    "\n",
    "Dict - Supports a dictionary of keys and subspaces, used for a fixed number of unordered spaces\n",
    "\n",
    "Tuple - Supports a tuple of subspaces, used for multiple for a fixed number of ordered spaces\n",
    "\n",
    "Sequence - Supports a variable number of instances of a single subspace, used for entities spaces or selecting a variable number of actions\n",
    "\n",
    "Graph - Supports graph based actions or observations with discrete or continuous nodes and edge values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9710aae",
   "metadata": {},
   "source": [
    "\n",
    "Example: if we want to build an observation of a PNG image , you can use the follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec3b55",
   "metadata": {},
   "source": [
    "\n",
    "    # The action and observation spaces need to be gym.spaces objects:\n",
    "    self.action_space = Discrete(4)  # up, left, right, down\n",
    "    # Here's an observation space for 200 wide x 100 high RGB image inputs:\n",
    "    self.observation_space = Box(\n",
    "        low=0, high=255, shape=(100, 200, 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2bc1c9",
   "metadata": {},
   "source": [
    "The standard structure of the ray  enviroment \n",
    "https://docs.ray.io/en/latest/rllib/rllib-env.html\n",
    "should be:\n",
    "\n",
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = <gym.Space>\n",
    "        self.observation_space = <gym.Space>\n",
    "    def reset(self):\n",
    "        return <obs>\n",
    "    def step(self, action):\n",
    "        return <obs>, <reward: float>, <done: bool>, <info: dict>\n",
    "\n",
    "ray.init()\n",
    "algo = ppo.PPO(env=MyEnv, config={\n",
    "    \"env_config\": {},  # config to pass to env class\n",
    "})\n",
    "\n",
    "while True:\n",
    "    print(algo.train())\n",
    "    \n",
    "    \n",
    "    \n",
    "https://docs.ray.io/en/latest/rllib/rllib-algorithms.html?highlight=%20APPOConfig()#appo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0953a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "from pygame import display\n",
    "from pygame.surfarray import array3d\n",
    "import random\n",
    "\n",
    "BLACK = pygame.Color(0, 0, 0)\n",
    "WHITE = pygame.Color(255, 255, 255)\n",
    "RED = pygame.Color(255, 0, 0)\n",
    "GREEN = pygame.Color(0, 255, 0)\n",
    "BLUE = pygame.Color(0, 0, 255)\n",
    "worker_pos=[0,0]\n",
    "\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "\n",
    "        # We inizialize the display\n",
    "        self.frame_size_x = 200\n",
    "        self.frame_size_y = 200\n",
    "        self.game_window = pygame.display.set_mode((self.frame_size_x, self.frame_size_y))   \n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects    \n",
    "            \n",
    "        # ------------------ACTION SPACE----------------------------------\n",
    "        # The action space are the possible actions that is allowed by the worker\n",
    "        # previously was a simple number \n",
    "        # self.action_space = spaces.Discrete(1)\n",
    "        # Now in this example a simple action is a single coordinates (x,y)      \n",
    "        self.action_space = gym.spaces.Box(low=0, high=200, shape=(2,), dtype=np.int32)#dtype=np.float32) \n",
    "        \n",
    "        \n",
    "        # ------------------OBSERVATION SPACE-------------------------------\n",
    "        # Is what the state  is observed\n",
    "        #Previously was a single number discrete\n",
    "        #self.observation_space = gym.spaces.Discrete(2)\n",
    "        # For this example we choose a numpy array   shape=(2,) or an image\n",
    "        # Attention!!! The observation space should have the same shape that the reset \n",
    "        # In order to perform the RL training otherwise will fail!!\n",
    "       \n",
    "        self.observation_space = spaces.Box(low=0, high=200,\n",
    "                                            shape=(2,), dtype=np.int32) #np.float32) \n",
    "        '''\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\"x_position\": gym.spaces.Box(low=0, high=6, shape=(1,), dtype=np.uint8),\n",
    "             \"y_position\": gym.spaces.Box(low=0, high=6, shape=(1,), dtype=np.uint8),\n",
    "             }\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        # Initial conditions\n",
    "        \n",
    "        self.game_window.fill(WHITE)\n",
    "        # Moreover we add a position in the screen display\n",
    "        \n",
    "        # Initial worker position\n",
    "        self.worker_pos=[0,0]\n",
    "        \n",
    "    \n",
    "        #We assing a kind of gym object to a circle \n",
    "        self.worker_rect=pygame.draw.circle(self.game_window,BLUE,(self.worker_pos[0], self.worker_pos[1]),6) # DRAW CIRCLE\n",
    "\n",
    "        # Initial target position\n",
    "        \n",
    "        self.target_pos = [100, 100]\n",
    "        print('Initial target position',self.target_pos[0],self.target_pos[1])\n",
    "        self.target_rect=pygame.draw.circle(self.game_window,RED,(self.target_pos[0], self.target_pos[1]),6) # DRAW CIRCLE\n",
    "        self.steps = 0\n",
    "\n",
    "    def reward_value(self,worker,target):\n",
    "         #Check for collision between two rects\n",
    "        if worker.colliderect(target):\n",
    "            #print(\"worker, target\",worker, target)\n",
    "            reward=1.0\n",
    "            done=True    \n",
    "        else:\n",
    "            reward=-1.0\n",
    "            done=False\n",
    "        return reward\n",
    "              \n",
    "    def step(self, action):\n",
    "        #reward = 0.0\n",
    "        \n",
    "        self.worker_pos = action\n",
    "        #print('worker_pos:'self.worker_pos)\n",
    "            \n",
    "        # We update the state with an image ( in other words plot the points due to the action)\n",
    "        self.update_game_state()\n",
    "\n",
    "        #print(self.worker_rect,self.target_rect)\n",
    "        \n",
    "        reward= self.reward_value(self.worker_rect,self.target_rect)\n",
    " \n",
    "        # regardless of the action, game is done after step becomes true\n",
    "        reward_tmp, done = self.game_over(reward)\n",
    "        \n",
    "        #self.reward=self.reward+reward_tmp\n",
    "        self.reward=reward_tmp\n",
    "        \n",
    "        \n",
    "        info = {}        \n",
    "        \n",
    "        \n",
    "        #Accumulative reward\n",
    "        print('Reward in step:',self.steps,self.reward)\n",
    "        \n",
    "        # -----Under the assuption that we deal with an image---\n",
    "        #img = self.get_image_array_from_game()\n",
    "        #state=img\n",
    "        #--------------------------------------------------------\n",
    "        self.state=[self.target_pos[0], self.target_pos[1]]\n",
    "\n",
    "        #print('step:', self.steps)\n",
    "        self.steps += 1\n",
    "        \n",
    "        observation = np.array(self.state, dtype=np.int32) #float32)\n",
    "        return observation, self.reward, done, info\n",
    "    \n",
    "    def worker_step(self,event):   \n",
    "        '''\n",
    "        Takes human keyboard event and then returns it as an action string\n",
    "        '''\n",
    "        action = None\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            sys.exit()\n",
    "            \n",
    "        #Move based on mouse clicks\n",
    "        if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "            #print(event)\n",
    "            mouse_x = event.pos[0]\n",
    "            mouse_y = event.pos[1]\n",
    "            self.worker_pos[0]=mouse_x\n",
    "            self.worker_pos[1]=mouse_y\n",
    "            action = [self.worker_pos[0], self.worker_pos[1]]\n",
    "        #Drag the object when the mouse button is clicked\n",
    "        \n",
    "        if event.type == pygame.MOUSEMOTION and event.buttons[0] == 1:\n",
    "            #print(event)\n",
    "            mouse_x = event.pos[0]\n",
    "            mouse_y = event.pos[1]\n",
    "            self.worker_pos[0]=mouse_x\n",
    "            self.worker_pos[1]=mouse_y\n",
    "            action = [self.worker_pos[0], self.worker_pos[1]]   \n",
    "        \n",
    "        elif event.type == pygame.KEYDOWN:\n",
    "            # Esc -> Create event to quit the game\n",
    "            if event.key == pygame.K_ESCAPE:\n",
    "                pygame.event.post(pygame.event.Event(pygame.QUIT))                \n",
    "        \n",
    "        return  action    \n",
    "    \n",
    "    def update_game_state(self):\n",
    "        \n",
    "        #We fill the screen to white\n",
    "        self.game_window.fill(WHITE)\n",
    "        \n",
    "        #Draw rectangles to represent the rect's of each object\n",
    "        # For the worker\n",
    "        \n",
    "        self.worker_rect.x=self.worker_pos[0]\n",
    "        self.worker_rect.y=self.worker_pos[1]\n",
    "        self.worker_rect=pygame.draw.circle(self.game_window,BLUE,(self.worker_rect.x,self.worker_rect.y),6) # DRAW CIRCLE\n",
    "        \n",
    "        # For the target\n",
    "        self.target_rect.x=self.target_pos[0]\n",
    "        self.target_rect.y=self.target_pos[1]        \n",
    "        pygame.draw.circle(self.game_window,RED,(self.target_rect.x,self.target_rect.y),6) # DRAW CIRCLE\n",
    "\n",
    "\n",
    "    def get_image_array_from_game(self):\n",
    "        img = array3d(display.get_surface())\n",
    "        #Preprocessing of channels ( needed for tensorflow)\n",
    "        img = np.swapaxes(img, 0, 1)\n",
    "        return img    \n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        # Type 1 Observation\n",
    "        #print('Target position',self.target_pos[0], self.target_pos[1])\n",
    "        #observation = np.array([self.target_pos[0], self.target_pos[1]])\n",
    "        \n",
    "        #Type 2 Observation\n",
    "        #We create a simple observation state from a sample\n",
    "        #self.state = self.observation_space.sample()\n",
    "        #observation = np.array(self.state, dtype=np.int32) #float32)\n",
    "\n",
    "        #Type 2 From a random sit\n",
    "        #sit_random = np.random.randint(1, 9)\n",
    "        #self.state['x_position'][sit_random:] = 0\n",
    "        #self.state['y_position'][sit_random:] = 0\n",
    "        #observation = self.state\n",
    "        \n",
    "        #Type 4 From a target\n",
    "        self.state=[self.target_pos[0], self.target_pos[1]]        \n",
    "        observation = np.array(self.state, dtype=np.int32) #float32)       \n",
    "        \n",
    "        # Type 3 Observation\n",
    "        #img = array3d(display.get_surface())\n",
    "        #img = np.swapaxes(img, 0, 1)\n",
    "        #observation=img\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.reward = 0\n",
    "\n",
    "        #print(\"Game Reset.\")\n",
    "        #print('observation',observation)\n",
    "        return observation    \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == \"human\":\n",
    "            display.update()        \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def game_over(self, reward):\n",
    "        if (reward < 1) or (self.steps >= 1000): \n",
    "            return -1, False\n",
    "        else:\n",
    "            return reward, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0adc5cb",
   "metadata": {},
   "source": [
    "# Ray testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "ray.init()\n",
    "algo = ppo.PPO(env=MyEnv, config={\n",
    "    \"env_config\": {},  # config to pass to env class\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173226d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ppo = []\n",
    "for _ in range(25):\n",
    "    result = algo.train()\n",
    "    print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "    mean_ppo.append(result['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da573600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = [x for x in range(len(mean_ppo))]\n",
    "\n",
    "plt.plot(xs, mean_ppo)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1a539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ef044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0448d61",
   "metadata": {},
   "source": [
    "# Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1 - use local test class\n",
    "# Testing local frame\n",
    "env = MyEnv(env_config={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87f3fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "print('action',action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb037bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#action=[100,100]\n",
    "state, reward, done, info = env.step(action)\n",
    "print(\"Reward = {} with action = {}\".format(reward,action))\n",
    "import matplotlib.pyplot as plt\n",
    "print(reward, done, info)\n",
    "#state = np.array(state)\n",
    "\n",
    "print(state,type(state))\n",
    "#plt.figure()\n",
    "#plt.imshow(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f727b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing multiple frames\n",
    "import sys\n",
    "env = MyEnv(env_config={})\n",
    "env.reset()\n",
    "# This is technically a FPS Refresh rate\n",
    "FPS = 10\n",
    "# FPS (frames per second) controller\n",
    "fps_controller = pygame.time.Clock()\n",
    "# Checks for errors encountered\n",
    "check_errors = pygame.init()\n",
    "# Initialise game window\n",
    "pygame.display.set_caption('Testing Game') \n",
    "#The main game loop\n",
    "running = True\n",
    "while running:\n",
    "    # Check Input from Human Step \n",
    "    for event in pygame.event.get():\n",
    "        action = env.worker_step(event)    \n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "        pygame.display.update()\n",
    "        if action != None :# (0,0) :# and reward!=0:\n",
    "            #print(action,type(action))\n",
    "            state, reward, done, info = env.step(action)\n",
    "            print(\"Reward = {} with action = {} , done = {}\".format(reward,action,done))\n",
    "            # Refresh game screen    \n",
    "    # Refresh rate\n",
    "    fps_controller.tick(FPS)\n",
    "    img = array3d(env.game_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b4158",
   "metadata": {},
   "source": [
    "# stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "#env = MyEnv()\n",
    "env = MyEnv(env_config={})\n",
    "\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import os\n",
    "import time\n",
    "import pygame\n",
    "from pygame.surfarray import array3d\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de15b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = f\"models/{int(time.time())}/\"\n",
    "logdir = f\"logs/{int(time.time())}/\"\n",
    "\n",
    "fps_controller = pygame.time.Clock()\n",
    "fps_controller.tick(60)\n",
    "\n",
    "# Checks for errors encountered\n",
    "pygame.init()\n",
    "\n",
    "# Initialise game window\n",
    "pygame.display.set_caption('Traning')\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74262379",
   "metadata": {},
   "outputs": [],
   "source": [
    "envrl = DummyVecEnv([lambda: Monitor(env,logdir,allow_early_resets=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d47577",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", env,verbose=1, tensorboard_log=logdir,n_epochs=40)\n",
    "#model = PPO(\"MultiInputPolicy\", envrl,verbose=1, tensorboard_log=logdir,n_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 100\n",
    "iters = 0\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*iters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f5a03",
   "metadata": {},
   "source": [
    "### Init Ray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "#from ray.rllib.env.env_context import EnvContext\n",
    "#from ray.rllib.algorithms import appo\n",
    "#from ray.rllib.algorithms.appo import APPOConfig\n",
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "    assert ray.is_initialized()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.appo import APPOConfig\n",
    "config = (\n",
    "    APPOConfig()\n",
    "    .rollouts(horizon=10000)\n",
    "    .environment(\n",
    "        MyEnv,\n",
    "        env_config={}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f1a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8cefe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ea54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabcfea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb55fa68",
   "metadata": {},
   "source": [
    "# ray.rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2074ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "#import ray.rllib.agents.ppo as ppo\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc93b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(ray)\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ca4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a52787",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = ppo.PPO(env=MyEnv, config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c87d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Dashboard URL: http://{}\".format(ray.get_webui_url()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = PPOConfig()#.rollouts(horizon=200) \n",
    "config={\n",
    "        \"env\": MyEnv,\n",
    "        \"num_workers\": 30,\n",
    "        \"num_cpus_per_worker\": 0.5,\n",
    "        \"env_config\":{\n",
    "            \"max_steps\": 1000,\n",
    "            \"export_frames\": False,\n",
    "            \"export_states\": False,\n",
    "            # \"reward_mode\": \"continuous\",\n",
    "            # \"env_flipped\": True,\n",
    "            # \"env_flipmode\": True,\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algo = ppo.PPO(env=MyEnv, config={\"env_config\": {},  # config to pass to env class\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc89f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ppo = []\n",
    "for _ in range(25):\n",
    "    result = algo.train()\n",
    "    print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "    mean_ppo.append(result['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "tune.run(\n",
    "    \"SAC\", # reinforced learning agent\n",
    "    name = \"Training2\",\n",
    "    checkpoint_freq = 100,\n",
    "    checkpoint_at_end = True,\n",
    "    local_dir = r'./ray_results/',\n",
    "    config={\n",
    "        \"env\": MyEnv,\n",
    "        \"num_workers\": 30,\n",
    "        \"num_cpus_per_worker\": 0.5,\n",
    "        \"env_config\":{\n",
    "            \"max_steps\": 1000,\n",
    "            \"export_frames\": False,\n",
    "            \"export_states\": False,\n",
    "            # \"reward_mode\": \"continuous\",\n",
    "            # \"env_flipped\": True,\n",
    "            # \"env_flipmode\": True,\n",
    "            }\n",
    "        },\n",
    "    stop = {\n",
    "        \"timesteps_total\": 5_000_000,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 30\n",
    "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} saved {}\"\n",
    "\n",
    "for n in range(N_ITER):\n",
    "  result = algo.train()\n",
    "  #file_name = agent.save(CHECKPOINT_ROOT)\n",
    "\n",
    "  print(s.format(\n",
    "    n + 1,\n",
    "    result[\"episode_reward_min\"],\n",
    "    result[\"episode_reward_mean\"],\n",
    "    result[\"episode_reward_max\"],\n",
    "    result[\"episode_len_mean\"],\n",
    "    file_name\n",
    "   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07855a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Gym)",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
