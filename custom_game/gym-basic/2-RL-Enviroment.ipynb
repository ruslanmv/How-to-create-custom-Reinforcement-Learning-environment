{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1363d0b8",
   "metadata": {},
   "source": [
    "# 3 - RL Enviroment \n",
    "In this part we are going to build the most essential Enviroment to create a RL  Pipeline.\n",
    "\n",
    "The first framework that we are going to use is the  **RAY**\n",
    "\n",
    "We are going to  pass either a string name or a Python class to specify an environment.  In particular we are going to choose the simplest local enviroment.\n",
    "\n",
    "Custom env classes passed directly to the algorithm must take a single env_config parameter in their constructor:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e085e",
   "metadata": {},
   "source": [
    "### Example 1 - Gym + Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32caadf4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scipy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m\u001b[38;5;241m,\u001b[39m \u001b[38;5;21;01mray\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgym\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m spaces\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01malgorithms\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ppo\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mMyEnv\u001b[39;00m(gym\u001b[38;5;241m.\u001b[39mEnv):\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, env_config\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m      7\u001b[0m        \u001b[38;5;66;03m# There are two actions, first will get reward of 1, second reward of -1. \u001b[39;00m\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\__init__.py:7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_private\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01musage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m usage_lib\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Note: do not introduce unnecessary library dependencies here, e.g. gym.\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# This file is imported from the tune module in order to register RLlib agents.\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseEnv\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExternalEnv\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_agent_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiAgentEnv\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\env\\__init__.py:6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal_multi_agent_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExternalMultiAgentEnv\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_agent_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiAgentEnv\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy_client\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolicyClient\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy_server_input\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PolicyServerInput\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mremote_base_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RemoteBaseEnv\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\env\\policy_client.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexternal_multi_agent_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ExternalMultiAgentEnv\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01menv\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmulti_agent_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiAgentEnv\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msample_batch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m MultiAgentBatch\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PublicAPI\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     MultiAgentDict,\n\u001b[0;32m     21\u001b[0m     EnvInfoDict,\n\u001b[0;32m     22\u001b[0m     EnvObsType,\n\u001b[0;32m     23\u001b[0m     EnvActionType,\n\u001b[0;32m     24\u001b[0m )\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\__init__.py:1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Policy\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_policy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TorchPolicy\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_policy\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFPolicy\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\policy.py:28\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mair\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcheckpoint\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Checkpoint\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcloudpickle\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maction_dist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ActionDistribution\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcatalog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCatalog\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelV2\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\models\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maction_dist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ActionDistribution\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcatalog\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCatalog, MODEL_DEFAULTS\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelV2\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Preprocessor\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\models\\catalog.py:18\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelV2\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessors\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_preprocessor, Preprocessor\n\u001b[1;32m---> 18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_action_dist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     19\u001b[0m     Categorical,\n\u001b[0;32m     20\u001b[0m     Deterministic,\n\u001b[0;32m     21\u001b[0m     DiagGaussian,\n\u001b[0;32m     22\u001b[0m     Dirichlet,\n\u001b[0;32m     23\u001b[0m     MultiActionDistribution,\n\u001b[0;32m     24\u001b[0m     MultiCategorical,\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtorch_action_dist\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     27\u001b[0m     TorchCategorical,\n\u001b[0;32m     28\u001b[0m     TorchDeterministic,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m     TorchMultiCategorical,\n\u001b[0;32m     32\u001b[0m )\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeveloperAPI, PublicAPI\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\models\\tf\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_modelv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFModelV2\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfcnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FullyConnectedNetwork\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrecurrent_net\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m RecurrentNetwork\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvisionnet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m VisionNetwork\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFullyConnectedNetwork\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRecurrentNetwork\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTFModelV2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVisionNetwork\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     11\u001b[0m ]\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\models\\tf\\recurrent_net.py:10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodelv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelV2\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtf_modelv2\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TFModelV2\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrnn_sequencing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m add_time_dimension\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msample_batch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SampleBatch\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mview_requirement\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ViewRequirement\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\policy\\rnn_sequencing.py:21\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpolicy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msample_batch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SampleBatch\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mannotations\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DeveloperAPI\n\u001b[1;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summarize\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m try_import_tf, try_import_torch\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TensorType, ViewRequirementsDict\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\utils\\debug\\__init__.py:2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeterministic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m update_global_seed_if_necessary\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmemory\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m check_memory_leaks\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mray\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrllib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdebug\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msummary\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m summarize\n\u001b[0;32m      6\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcheck_memory_leaks\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummarize\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdate_global_seed_if_necessary\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     10\u001b[0m ]\n",
      "File \u001b[1;32m~\\.conda\\envs\\gym\\lib\\site-packages\\ray\\rllib\\utils\\debug\\memory.py:5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtracemalloc\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtree\u001b[39;00m  \u001b[38;5;66;03m# pip install dm_tree\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'scipy'"
     ]
    }
   ],
   "source": [
    "import gym, ray\n",
    "from gym import spaces\n",
    "from ray.rllib.algorithms import ppo\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "       # There are two actions, first will get reward of 1, second reward of -1. \n",
    "        self.action_space = spaces.Discrete(5)      #<gym.Space>\n",
    "        self.observation_space = spaces.Discrete(2) #<gym.Space>\n",
    "    \n",
    "    def reset(self):\n",
    "        state = 0\n",
    "        #return <obs>\n",
    "        return state\n",
    "                           \n",
    "    def step(self, action):\n",
    "\n",
    "        # if we took an action, we were in state 1\n",
    "        state = 1\n",
    "    \n",
    "        if action == 2:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        # regardless of the action, game is done after a single step\n",
    "        done = True\n",
    "\n",
    "        info = {}\n",
    "        # return <obs>, <reward: float>, <done: bool>, <info: dict>\n",
    "        return state, reward, done, info   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b96cc",
   "metadata": {},
   "source": [
    "Python 3.8.x\n",
    "ray 1.0\n",
    "tensorflow 2.3.1\n",
    "tensorflow-probability 0.11\n",
    "gym 0.17.3\n",
    "pygame 2.0.0\n",
    "\n",
    "numpy==1.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8be9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ec2e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "tune.run(\n",
    "    \"SAC\", # reinforced learning agent\n",
    "    name = \"Training1\",\n",
    "    checkpoint_freq = 100,\n",
    "    checkpoint_at_end = True,\n",
    "    local_dir = r'./ray_results/',\n",
    "    config={\n",
    "        \"env\": MyEnv,\n",
    "        \"num_workers\": 30,\n",
    "        \"num_cpus_per_worker\": 0.5,\n",
    "        \"env_config\":{\n",
    "            \"max_steps\": 100,\n",
    "            \"export_frames\": False,\n",
    "            \"export_states\": False,\n",
    "            # \"reward_mode\": \"continuous\",\n",
    "            # \"env_flipped\": True,\n",
    "            # \"env_flipmode\": True,\n",
    "            }\n",
    "        },\n",
    "    stop = {\n",
    "        \"timesteps_total\": 5_000,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bd1d32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray.init()\n",
    "#algo = ppo.PPO(env=MyEnv, config={\"env_config\": {},  # config to pass to env class\n",
    "#})\n",
    "\n",
    "#algo = ppo.PPO(env=MyEnv, config=config) \n",
    "\n",
    "algo = ppo.PPO(env=MyEnv,config={\"num_workers\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae30180",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ppo = []\n",
    "for _ in range(25):\n",
    "    result = algo.train()\n",
    "    print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "    mean_ppo.append(result['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1bafb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = [x for x in range(len(mean_ppo))]\n",
    "\n",
    "plt.plot(xs, mean_ppo)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63087033",
   "metadata": {},
   "source": [
    "### How to use the trained algorithm in RL with PP0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1565fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa82a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e56c0dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc021f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Fix the windows path\n",
    "#evaluation = trainer.evaluate(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a91c8b",
   "metadata": {},
   "source": [
    "## Computing actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d32737",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f4d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4784046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1207b318",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(observations) # The state which you should determine the action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0f9a8",
   "metadata": {},
   "source": [
    "Given any state compute the action which you get the maximum reward in according to the traning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8014b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = trainer.compute_single_action(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843a04b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1bade5",
   "metadata": {},
   "outputs": [],
   "source": [
    "while not done:\n",
    "    action = trainer.compute_single_action(observations)\n",
    "    observations, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(\"observations, reward, done, info\",observations, reward, done, info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "805eca4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = trainer.compute_actions({\"obs_1\": observations, \"obs_2\": observations})\n",
    "print(action)\n",
    "# {'obs_1': 0, 'ob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630100cd",
   "metadata": {},
   "source": [
    "In the following rl test we are going to use  stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77ffc67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pygame\n",
    "from pygame.surfarray import array3d\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = MyEnv()\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5eb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = f\"models/{int(time.time())}/\"\n",
    "logdir = f\"logs/{int(time.time())}/\"\n",
    "\n",
    "fps_controller = pygame.time.Clock()\n",
    "fps_controller.tick(60)\n",
    "\n",
    "# Checks for errors encountered\n",
    "pygame.init()\n",
    "\n",
    "# Initialise game window\n",
    "pygame.display.set_caption('Training')\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "\n",
    "env = DummyVecEnv([lambda: Monitor(MyEnv(),logdir,allow_early_resets=True)])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env,verbose=1, tensorboard_log=logdir,n_epochs=10)\n",
    "\n",
    "TIMESTEPS = 100\n",
    "iters = 0\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a5522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "473615a2",
   "metadata": {},
   "source": [
    "In the following example we are interested to include pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93878ceb",
   "metadata": {},
   "source": [
    "### Example 2 - Gym + Ray + Pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75cc0f",
   "metadata": {},
   "source": [
    "In order to create an appropiate gym enviroment applied to ray and pygame we need need to pay attention into two gym objects:\n",
    "\n",
    "-- action (object): The action to be performed in the step() function. \n",
    "In a game of chess, the action would be the specific, legal move performed by a player.\n",
    "\n",
    "--observation (object): This is all the information available to the agent to choose the next action. \n",
    "The observation is based only on the current state of the environment.\n",
    "\n",
    "## Observation_space and Action_space\n",
    "In  particular  the observation_space and action_space:\n",
    "there are only certain actions and observations are valid in regards of a specific environment. \n",
    "\n",
    "To define a format, the observation_space and action_space variables need to be assigned to a respective gym.space class.\n",
    "\n",
    "Spaces can differ in their dimensionality and their value range. Continuous and discrete spaces are both possible.\n",
    "\n",
    "self.observation_space = <gym.space>\n",
    "self.action_space = <gym.space>\n",
    "\n",
    "\n",
    "We are going to consider an enviroment where there are two\n",
    "points, one red and one blu. The purpose of the game is give a blue point ( worker) where intercept the red point (target)\n",
    "\n",
    "## Definition of action space\n",
    "\n",
    "We want to control the position of the blue point.\n",
    "\n",
    "So the action is the position, the action are the coordinates that you provides to the enviroment\n",
    "\n",
    "action =[x, y]\n",
    "\n",
    "The value of each coordinate are continous and must be in the range of the size of the horizontal box\n",
    "\n",
    "gym.spaces.Box(low=min_x., high=max_x., shape=(2,), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a87e7",
   "metadata": {},
   "source": [
    "# Analysis of Spaces\n",
    "Before we continue le us check some examples of spaces in order to understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4eb6f",
   "metadata": {},
   "source": [
    "## Box\n",
    "\n",
    "Box - Supports continuous (and discrete) vectors or matrices, used for vector observations, images, etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791065db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box, Discrete,MultiBinary , MultiDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "observation_space = Box(low=-1.0, high=2.0, shape=(3, 4), dtype=np.float32)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301722b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2\n",
    "observation_space = Box(low=np.array([-1.0, -2.0]), high=np.array([2.0, 4.0]), dtype=np.float32)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2\n",
    "observation_space = Box(low=np.array(-1.0), high=np.array(2.0), dtype=np.float32)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c906d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "observation_space = Box(low=0, high=200, shape=(2,), dtype=np.float32)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240269e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(observation_space.sample()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491b366",
   "metadata": {},
   "source": [
    "## Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceba308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 3\n",
    "observation_space =Discrete(2)            # {0, 1}\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 4\n",
    "observation_space =Discrete(3)  # {0, 1, 2}\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a723878f",
   "metadata": {},
   "source": [
    "## MultiBinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04342033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5\n",
    "observation_space = MultiBinary(5)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5\n",
    "observation_space = MultiBinary(2)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e85da7",
   "metadata": {},
   "source": [
    "# MultiDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6\n",
    "observation_space =  MultiDiscrete(np.array([[1, 2], [3, 4]]))\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade45ca",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6a\n",
    "#observation_space =Text(5)\n",
    "# {\"0\", \"42\", \"0123456789\", ...}\n",
    "#observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6b\n",
    "#import string\n",
    "#observation_space = Text(min_length = 1,\n",
    "#     max_length = 10,\n",
    "#     charset = string.digits)\n",
    "#observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe9905",
   "metadata": {},
   "source": [
    "# Dict\n",
    "Elements of this space are (ordered) dictionaries of elements from the constituent spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7\n",
    "from gym.spaces import Dict, Discrete\n",
    "observation_space = Dict({\"position\": Discrete(2), \"velocity\": Discrete(3)})\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 8 [nested]:\n",
    "from gym.spaces import Box, Dict, Discrete, MultiBinary, MultiDiscrete\n",
    "observation_space =Dict(\n",
    "    {\n",
    "        \"ext_controller\": MultiDiscrete([5, 2, 2]),\n",
    "        \"inner_state\": Dict(\n",
    "            {\n",
    "                \"charge\": Discrete(100),\n",
    "                \"system_checks\": MultiBinary(10),\n",
    "                \"job_status\": Dict(\n",
    "                    {\n",
    "                        \"task\": Discrete(5),\n",
    "                        \"progress\": Box(low=0, high=100, shape=()),\n",
    "                    }\n",
    "                ),\n",
    "            }\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 9\n",
    "from gym.spaces import Box, Discrete\n",
    "observation_space = Dict({\"position\": Box(-1, 1, shape=(2,)), \"color\": Discrete(3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07829e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('seats_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3181278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#id  x, y z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba961110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle(\"./employees.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791152fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00191e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 10\n",
    "observation_space = = gym.spaces.Dict(\n",
    "    {\"x_position\": gym.spaces.Box(low=0, high=6, shape=(self.max_sit,), dtype=np.uint8),\n",
    "     \"y_position\": gym.spaces.Box(low=0, high=6, shape=(self.max_sit,), dtype=np.uint8),\n",
    "     \"cluster\": gym.spaces.Box(low=0, high=6, shape=(self.max_sit,), dtype=np.uint8),\n",
    "     \"project\": gym.spaces.Box(low=0, high=6, shape=(self.max_sit,), dtype=np.uint8),\n",
    "     \"energy_consumption\": gym.spaces.Box(low=0, high=1, shape=(self.max_sit,)),\n",
    "     \"emp_project\": gym.spaces.Box(low=0, high=6, shape=(1,), dtype=np.int32),\n",
    "     \"emp_energy_consumption\": gym.spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b425077",
   "metadata": {},
   "source": [
    "# Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d539ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 10\n",
    "from gym.spaces import Box, Discrete, Tuple\n",
    "observation_space = Tuple((Discrete(2), Box(-1, 1, shape=(2,))))\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe075c02",
   "metadata": {},
   "source": [
    "# Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc90c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 11\n",
    "#from gym.spaces import Sequence\n",
    "#space = Sequence(Box(0, 1))\n",
    "#space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71d349",
   "metadata": {},
   "source": [
    "for futher information visit\n",
    "https://gymnasium.farama.org/api/spaces/composite/#gymnasium.spaces.Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb061b",
   "metadata": {},
   "source": [
    "# ---- Summary---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4396bf",
   "metadata": {},
   "source": [
    "Discrete - Supports a single discrete number of values with an optional start for the values\n",
    "\n",
    "MultiDiscrete - Supports single or matrices of binary values, used for holding down a button or if an agent has an object\n",
    "\n",
    "MultiBinary - Supports multiple discrete values with multiple axes, used for controller actions\n",
    "\n",
    "Text - Supports strings, used for passing agent messages, mission details, etc\n",
    "\n",
    "Composite Spaces\n",
    "Often environment spaces require joining fundamental spaces together for vectorised environments, separate agents or readability of the space.\n",
    "\n",
    "Dict - Supports a dictionary of keys and subspaces, used for a fixed number of unordered spaces\n",
    "\n",
    "Tuple - Supports a tuple of subspaces, used for multiple for a fixed number of ordered spaces\n",
    "\n",
    "Sequence - Supports a variable number of instances of a single subspace, used for entities spaces or selecting a variable number of actions\n",
    "\n",
    "Graph - Supports graph based actions or observations with discrete or continuous nodes and edge values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9710aae",
   "metadata": {},
   "source": [
    "\n",
    "Example: if we want to build an observation of a PNG image , you can use the follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec3b55",
   "metadata": {},
   "source": [
    "\n",
    "    # The action and observation spaces need to be gym.spaces objects:\n",
    "    self.action_space = Discrete(4)  # up, left, right, down\n",
    "    # Here's an observation space for 200 wide x 100 high RGB image inputs:\n",
    "    self.observation_space = Box(\n",
    "        low=0, high=255, shape=(100, 200, 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2bc1c9",
   "metadata": {},
   "source": [
    "The standard structure of the ray  enviroment \n",
    "https://docs.ray.io/en/latest/rllib/rllib-env.html\n",
    "should be:\n",
    "\n",
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = <gym.Space>\n",
    "        self.observation_space = <gym.Space>\n",
    "    def reset(self):\n",
    "        return <obs>\n",
    "    def step(self, action):\n",
    "        return <obs>, <reward: float>, <done: bool>, <info: dict>\n",
    "\n",
    "ray.init()\n",
    "algo = ppo.PPO(env=MyEnv, config={\n",
    "    \"env_config\": {},  # config to pass to env class\n",
    "})\n",
    "\n",
    "while True:\n",
    "    print(algo.train())\n",
    "    \n",
    "    \n",
    "    \n",
    "https://docs.ray.io/en/latest/rllib/rllib-algorithms.html?highlight=%20APPOConfig()#appo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0953a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "from pygame import display\n",
    "from pygame.surfarray import array3d\n",
    "import random\n",
    "\n",
    "BLACK = pygame.Color(0, 0, 0)\n",
    "WHITE = pygame.Color(255, 255, 255)\n",
    "RED = pygame.Color(255, 0, 0)\n",
    "GREEN = pygame.Color(0, 255, 0)\n",
    "BLUE = pygame.Color(0, 0, 255)\n",
    "worker_pos=[0,0]\n",
    "\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "\n",
    "        # We inizialize the display\n",
    "        self.frame_size_x = 200\n",
    "        self.frame_size_y = 200\n",
    "        self.game_window = pygame.display.set_mode((self.frame_size_x, self.frame_size_y))   \n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects    \n",
    "            \n",
    "        # ------------------ACTION SPACE----------------------------------\n",
    "        # The action space are the possible actions that is allowed by the worker\n",
    "        # previously was a simple number \n",
    "        # self.action_space = spaces.Discrete(1)\n",
    "        # Now in this example a simple action is a single coordinates (x,y)      \n",
    "        self.action_space = gym.spaces.Box(low=0, high=200, shape=(2,), dtype=np.int32)#dtype=np.float32) \n",
    "        \n",
    "        \n",
    "        # ------------------OBSERVATION SPACE-------------------------------\n",
    "        # Is what the state  is observed\n",
    "        #Previously was a single number discrete\n",
    "        #self.observation_space = gym.spaces.Discrete(2)\n",
    "        # For this example we choose a numpy array   shape=(2,) or an image\n",
    "        # Attention!!! The observation space should have the same shape that the reset \n",
    "        # In order to perform the RL training otherwise will fail!!\n",
    "       \n",
    "        self.observation_space = spaces.Box(low=0, high=200,\n",
    "                                            shape=(2,), dtype=np.int32) #np.float32) \n",
    "        '''\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\"x_position\": gym.spaces.Box(low=0, high=6, shape=(1,), dtype=np.uint8),\n",
    "             \"y_position\": gym.spaces.Box(low=0, high=6, shape=(1,), dtype=np.uint8),\n",
    "             }\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        # Initial conditions\n",
    "        \n",
    "        self.game_window.fill(WHITE)\n",
    "        # Moreover we add a position in the screen display\n",
    "        \n",
    "        # Initial worker position\n",
    "        self.worker_pos=[0,0]\n",
    "        \n",
    "    \n",
    "        #We assing a kind of gym object to a circle \n",
    "        self.worker_rect=pygame.draw.circle(self.game_window,BLUE,(self.worker_pos[0], self.worker_pos[1]),6) # DRAW CIRCLE\n",
    "\n",
    "        # Initial target position\n",
    "        \n",
    "        self.target_pos = [100, 100]\n",
    "        print('Initial target position',self.target_pos[0],self.target_pos[1])\n",
    "        self.target_rect=pygame.draw.circle(self.game_window,RED,(self.target_pos[0], self.target_pos[1]),6) # DRAW CIRCLE\n",
    "        self.steps = 0\n",
    "\n",
    "    def reward_value(self,worker,target):\n",
    "         #Check for collision between two rects\n",
    "        if worker.colliderect(target):\n",
    "            #print(\"worker, target\",worker, target)\n",
    "            reward=1.0\n",
    "            done=True    \n",
    "        else:\n",
    "            reward=-1.0\n",
    "            done=False\n",
    "        return reward\n",
    "              \n",
    "    def step(self, action):\n",
    "        #reward = 0.0\n",
    "        \n",
    "        self.worker_pos = action\n",
    "        #print('worker_pos:'self.worker_pos)\n",
    "            \n",
    "        # We update the state with an image ( in other words plot the points due to the action)\n",
    "        self.update_game_state()\n",
    "\n",
    "        #print(self.worker_rect,self.target_rect)\n",
    "        \n",
    "        reward= self.reward_value(self.worker_rect,self.target_rect)\n",
    " \n",
    "        # regardless of the action, game is done after step becomes true\n",
    "        reward_tmp, done = self.game_over(reward)\n",
    "        \n",
    "        #self.reward=self.reward+reward_tmp\n",
    "        self.reward=reward_tmp\n",
    "        \n",
    "        \n",
    "        info = {}        \n",
    "        \n",
    "        \n",
    "        #Accumulative reward\n",
    "        print('Reward in step:',self.steps,self.reward)\n",
    "        \n",
    "        # -----Under the assuption that we deal with an image---\n",
    "        #img = self.get_image_array_from_game()\n",
    "        #state=img\n",
    "        #--------------------------------------------------------\n",
    "        self.state=[self.target_pos[0], self.target_pos[1]]\n",
    "\n",
    "        #print('step:', self.steps)\n",
    "        self.steps += 1\n",
    "        \n",
    "        observation = np.array(self.state, dtype=np.int32) #float32)\n",
    "        return observation, self.reward, done, info\n",
    "    \n",
    "    def worker_step(self,event):   \n",
    "        '''\n",
    "        Takes human keyboard event and then returns it as an action string\n",
    "        '''\n",
    "        action = None\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            sys.exit()\n",
    "            \n",
    "        #Move based on mouse clicks\n",
    "        if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "            #print(event)\n",
    "            mouse_x = event.pos[0]\n",
    "            mouse_y = event.pos[1]\n",
    "            self.worker_pos[0]=mouse_x\n",
    "            self.worker_pos[1]=mouse_y\n",
    "            action = [self.worker_pos[0], self.worker_pos[1]]\n",
    "        #Drag the object when the mouse button is clicked\n",
    "        \n",
    "        if event.type == pygame.MOUSEMOTION and event.buttons[0] == 1:\n",
    "            #print(event)\n",
    "            mouse_x = event.pos[0]\n",
    "            mouse_y = event.pos[1]\n",
    "            self.worker_pos[0]=mouse_x\n",
    "            self.worker_pos[1]=mouse_y\n",
    "            action = [self.worker_pos[0], self.worker_pos[1]]   \n",
    "        \n",
    "        elif event.type == pygame.KEYDOWN:\n",
    "            # Esc -> Create event to quit the game\n",
    "            if event.key == pygame.K_ESCAPE:\n",
    "                pygame.event.post(pygame.event.Event(pygame.QUIT))                \n",
    "        \n",
    "        return  action    \n",
    "    \n",
    "    def update_game_state(self):\n",
    "        \n",
    "        #We fill the screen to white\n",
    "        self.game_window.fill(WHITE)\n",
    "        \n",
    "        #Draw rectangles to represent the rect's of each object\n",
    "        # For the worker\n",
    "        \n",
    "        self.worker_rect.x=self.worker_pos[0]\n",
    "        self.worker_rect.y=self.worker_pos[1]\n",
    "        self.worker_rect=pygame.draw.circle(self.game_window,BLUE,(self.worker_rect.x,self.worker_rect.y),6) # DRAW CIRCLE\n",
    "        \n",
    "        # For the target\n",
    "        self.target_rect.x=self.target_pos[0]\n",
    "        self.target_rect.y=self.target_pos[1]        \n",
    "        pygame.draw.circle(self.game_window,RED,(self.target_rect.x,self.target_rect.y),6) # DRAW CIRCLE\n",
    "\n",
    "\n",
    "    def get_image_array_from_game(self):\n",
    "        img = array3d(display.get_surface())\n",
    "        #Preprocessing of channels ( needed for tensorflow)\n",
    "        img = np.swapaxes(img, 0, 1)\n",
    "        return img    \n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        # Type 1 Observation\n",
    "        #print('Target position',self.target_pos[0], self.target_pos[1])\n",
    "        #observation = np.array([self.target_pos[0], self.target_pos[1]])\n",
    "        \n",
    "        #Type 2 Observation\n",
    "        #We create a simple observation state from a sample\n",
    "        #self.state = self.observation_space.sample()\n",
    "        #observation = np.array(self.state, dtype=np.int32) #float32)\n",
    "\n",
    "        #Type 2 From a random sit\n",
    "        #sit_random = np.random.randint(1, 9)\n",
    "        #self.state['x_position'][sit_random:] = 0\n",
    "        #self.state['y_position'][sit_random:] = 0\n",
    "        #observation = self.state\n",
    "        \n",
    "        #Type 4 From a target\n",
    "        self.state=[self.target_pos[0], self.target_pos[1]]        \n",
    "        observation = np.array(self.state, dtype=np.int32) #float32)       \n",
    "        \n",
    "        # Type 3 Observation\n",
    "        #img = array3d(display.get_surface())\n",
    "        #img = np.swapaxes(img, 0, 1)\n",
    "        #observation=img\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.reward = 0\n",
    "\n",
    "        #print(\"Game Reset.\")\n",
    "        #print('observation',observation)\n",
    "        return observation    \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == \"human\":\n",
    "            display.update()        \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def game_over(self, reward):\n",
    "        if (reward < 1) or (self.steps >= 1000): \n",
    "            return -1, False\n",
    "        else:\n",
    "            return reward, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0adc5cb",
   "metadata": {},
   "source": [
    "# Ray testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "ray.init()\n",
    "algo = ppo.PPO(env=MyEnv, config={\n",
    "    \"env_config\": {},  # config to pass to env class\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173226d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ppo = []\n",
    "for _ in range(25):\n",
    "    result = algo.train()\n",
    "    print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "    mean_ppo.append(result['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da573600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = [x for x in range(len(mean_ppo))]\n",
    "\n",
    "plt.plot(xs, mean_ppo)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1a539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ef044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0448d61",
   "metadata": {},
   "source": [
    "# Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1 - use local test class\n",
    "# Testing local frame\n",
    "env = MyEnv(env_config={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87f3fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "print('action',action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb037bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#action=[100,100]\n",
    "state, reward, done, info = env.step(action)\n",
    "print(\"Reward = {} with action = {}\".format(reward,action))\n",
    "import matplotlib.pyplot as plt\n",
    "print(reward, done, info)\n",
    "#state = np.array(state)\n",
    "\n",
    "print(state,type(state))\n",
    "#plt.figure()\n",
    "#plt.imshow(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f727b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing multiple frames\n",
    "import sys\n",
    "env = MyEnv(env_config={})\n",
    "env.reset()\n",
    "# This is technically a FPS Refresh rate\n",
    "FPS = 10\n",
    "# FPS (frames per second) controller\n",
    "fps_controller = pygame.time.Clock()\n",
    "# Checks for errors encountered\n",
    "check_errors = pygame.init()\n",
    "# Initialise game window\n",
    "pygame.display.set_caption('Testing Game') \n",
    "#The main game loop\n",
    "running = True\n",
    "while running:\n",
    "    # Check Input from Human Step \n",
    "    for event in pygame.event.get():\n",
    "        action = env.worker_step(event)    \n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "        pygame.display.update()\n",
    "        if action != None :# (0,0) :# and reward!=0:\n",
    "            #print(action,type(action))\n",
    "            state, reward, done, info = env.step(action)\n",
    "            print(\"Reward = {} with action = {} , done = {}\".format(reward,action,done))\n",
    "            # Refresh game screen    \n",
    "    # Refresh rate\n",
    "    fps_controller.tick(FPS)\n",
    "    img = array3d(env.game_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b4158",
   "metadata": {},
   "source": [
    "# stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "#env = MyEnv()\n",
    "env = MyEnv(env_config={})\n",
    "\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import os\n",
    "import time\n",
    "import pygame\n",
    "from pygame.surfarray import array3d\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de15b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = f\"models/{int(time.time())}/\"\n",
    "logdir = f\"logs/{int(time.time())}/\"\n",
    "\n",
    "fps_controller = pygame.time.Clock()\n",
    "fps_controller.tick(60)\n",
    "\n",
    "# Checks for errors encountered\n",
    "pygame.init()\n",
    "\n",
    "# Initialise game window\n",
    "pygame.display.set_caption('Traning')\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74262379",
   "metadata": {},
   "outputs": [],
   "source": [
    "envrl = DummyVecEnv([lambda: Monitor(env,logdir,allow_early_resets=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d47577",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", env,verbose=1, tensorboard_log=logdir,n_epochs=40)\n",
    "#model = PPO(\"MultiInputPolicy\", envrl,verbose=1, tensorboard_log=logdir,n_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 100\n",
    "iters = 0\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*iters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f5a03",
   "metadata": {},
   "source": [
    "### Init Ray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "#from ray.rllib.env.env_context import EnvContext\n",
    "#from ray.rllib.algorithms import appo\n",
    "#from ray.rllib.algorithms.appo import APPOConfig\n",
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "    assert ray.is_initialized()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.appo import APPOConfig\n",
    "config = (\n",
    "    APPOConfig()\n",
    "    .rollouts(horizon=10000)\n",
    "    .environment(\n",
    "        MyEnv,\n",
    "        env_config={}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f1a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8cefe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ea54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabcfea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb55fa68",
   "metadata": {},
   "source": [
    "# ray.rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2074ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "#import ray.rllib.agents.ppo as ppo\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc93b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(ray)\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ca4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a52787",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = ppo.PPO(env=MyEnv, config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c87d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Dashboard URL: http://{}\".format(ray.get_webui_url()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = PPOConfig()#.rollouts(horizon=200) \n",
    "config={\n",
    "        \"env\": MyEnv,\n",
    "        \"num_workers\": 30,\n",
    "        \"num_cpus_per_worker\": 0.5,\n",
    "        \"env_config\":{\n",
    "            \"max_steps\": 1000,\n",
    "            \"export_frames\": False,\n",
    "            \"export_states\": False,\n",
    "            # \"reward_mode\": \"continuous\",\n",
    "            # \"env_flipped\": True,\n",
    "            # \"env_flipmode\": True,\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algo = ppo.PPO(env=MyEnv, config={\"env_config\": {},  # config to pass to env class\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc89f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ppo = []\n",
    "for _ in range(25):\n",
    "    result = algo.train()\n",
    "    print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "    mean_ppo.append(result['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "tune.run(\n",
    "    \"SAC\", # reinforced learning agent\n",
    "    name = \"Training2\",\n",
    "    checkpoint_freq = 100,\n",
    "    checkpoint_at_end = True,\n",
    "    local_dir = r'./ray_results/',\n",
    "    config={\n",
    "        \"env\": MyEnv,\n",
    "        \"num_workers\": 30,\n",
    "        \"num_cpus_per_worker\": 0.5,\n",
    "        \"env_config\":{\n",
    "            \"max_steps\": 1000,\n",
    "            \"export_frames\": False,\n",
    "            \"export_states\": False,\n",
    "            # \"reward_mode\": \"continuous\",\n",
    "            # \"env_flipped\": True,\n",
    "            # \"env_flipmode\": True,\n",
    "            }\n",
    "        },\n",
    "    stop = {\n",
    "        \"timesteps_total\": 5_000_000,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 30\n",
    "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} saved {}\"\n",
    "\n",
    "for n in range(N_ITER):\n",
    "  result = algo.train()\n",
    "  #file_name = agent.save(CHECKPOINT_ROOT)\n",
    "\n",
    "  print(s.format(\n",
    "    n + 1,\n",
    "    result[\"episode_reward_min\"],\n",
    "    result[\"episode_reward_mean\"],\n",
    "    result[\"episode_reward_max\"],\n",
    "    result[\"episode_len_mean\"],\n",
    "    file_name\n",
    "   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07855a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Gym)",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
