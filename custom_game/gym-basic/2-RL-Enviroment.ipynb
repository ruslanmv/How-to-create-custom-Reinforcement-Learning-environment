{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1363d0b8",
   "metadata": {},
   "source": [
    "# 3 - RL Enviroment \n",
    "In this part we are going to build the most essential Enviroment to create a RL  Pipeline.\n",
    "\n",
    "The first framework that we are going to use is the  **RAY**\n",
    "\n",
    "We are going to  pass either a string name or a Python class to specify an environment.  In particular we are going to choose the simplest local enviroment.\n",
    "\n",
    "Custom env classes passed directly to the algorithm must take a single env_config parameter in their constructor:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360e085e",
   "metadata": {},
   "source": [
    "### Example 1 - Gym + Ray"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "32caadf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "from gym import spaces\n",
    "from ray.rllib.algorithms import ppo\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config=None):\n",
    "       # There are two actions, first will get reward of 1, second reward of -1. \n",
    "        self.action_space = spaces.Discrete(5)      #<gym.Space>\n",
    "        self.observation_space = spaces.Discrete(2) #<gym.Space>\n",
    "    \n",
    "    def reset(self):\n",
    "        state = 0\n",
    "        #return <obs>\n",
    "        return state\n",
    "                           \n",
    "    def step(self, action):\n",
    "\n",
    "        # if we took an action, we were in state 1\n",
    "        state = 1\n",
    "    \n",
    "        if action == 2:\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "            \n",
    "        # regardless of the action, game is done after a single step\n",
    "        done = True\n",
    "\n",
    "        info = {}\n",
    "        # return <obs>, <reward: float>, <done: bool>, <info: dict>\n",
    "        return state, reward, done, info   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2b96cc",
   "metadata": {},
   "source": [
    "Python 3.8.x\n",
    "ray 1.0\n",
    "tensorflow 2.3.1\n",
    "tensorflow-probability 0.11\n",
    "gym 0.17.3\n",
    "pygame 2.0.0\n",
    "\n",
    "numpy==1.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c8be9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9469dc",
   "metadata": {},
   "source": [
    "tune.run(\n",
    "    \"SAC\", # reinforced learning agent\n",
    "    name = \"Training1\",\n",
    "    checkpoint_freq = 100,\n",
    "    checkpoint_at_end = True,\n",
    "    local_dir = r'./ray_results/',\n",
    "    config={\n",
    "        \"env\": MyEnv,\n",
    "        \"num_workers\": 30,\n",
    "        \"num_cpus_per_worker\": 0.5,\n",
    "        \"env_config\":{\n",
    "            \"max_steps\": 100,\n",
    "            \"export_frames\": False,\n",
    "            \"export_states\": False,\n",
    "            # \"reward_mode\": \"continuous\",\n",
    "            # \"env_flipped\": True,\n",
    "            # \"env_flipmode\": True,\n",
    "            }\n",
    "        },\n",
    "    stop = {\n",
    "        \"timesteps_total\": 5_000,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bd1d32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-02-03 23:03:09,613\tINFO worker.py:1538 -- Started a local Ray instance.\n",
      "2023-02-03 23:03:12,773\tWARNING deprecation.py:47 -- DeprecationWarning: `algo = Algorithm(env='<class '__main__.MyEnv'>', ...)` has been deprecated. Use `algo = AlgorithmConfig().environment('<class '__main__.MyEnv'>').build()` instead. This will raise an error in the future!\n",
      "2023-02-03 23:03:12,774\tINFO algorithm_config.py:2503 -- Your framework setting is 'tf', meaning you are using static-graph mode. Set framework='tf2' to enable eager execution with tf2.x. You may also then want to set eager_tracing=True in order to reach similar execution speed as with static-graph mode.\n",
      "2023-02-03 23:03:12,825\tINFO algorithm.py:501 -- Current log_level is WARN. For more information, set 'log_level': 'INFO' / 'DEBUG' or use the -v and -vv flags.\n",
      "\u001b[2m\u001b[36m(RolloutWorker pid=17184)\u001b[0m 2023-02-03 23:03:20,551\tWARNING env.py:147 -- Your env doesn't have a .spec.max_episode_steps attribute. This is fine if you have set 'horizon' in your config dictionary, or `soft_horizon`. However, if you haven't, 'horizon' will default to infinity, and your environment will not be reset.\n",
      "2023-02-03 23:03:22,820\tWARNING util.py:66 -- Install gputil for GPU system monitoring.\n"
     ]
    }
   ],
   "source": [
    "ray.init()\n",
    "#algo = ppo.PPO(env=MyEnv, config={\"env_config\": {},  # config to pass to env class\n",
    "#})\n",
    "\n",
    "#algo = ppo.PPO(env=MyEnv, config=config) \n",
    "\n",
    "algo = ppo.PPO(env=MyEnv,config={\"num_workers\": 4})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae30180",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode reward mean: 0 -0.601\n",
      "episode reward mean: 1 -0.0915\n",
      "episode reward mean: 2 0.2275\n",
      "episode reward mean: 3 0.58\n",
      "episode reward mean: 4 0.7375\n",
      "episode reward mean: 5 0.85\n",
      "episode reward mean: 6 0.8875\n",
      "episode reward mean: 7 0.926\n",
      "episode reward mean: 8 0.942\n",
      "episode reward mean: 9 0.965\n",
      "episode reward mean: 10 0.983\n",
      "episode reward mean: 11 0.9935\n",
      "episode reward mean: 12 0.998\n",
      "episode reward mean: 13 1.0\n",
      "episode reward mean: 14 0.999\n",
      "episode reward mean: 15 0.9995\n",
      "episode reward mean: 16 1.0\n",
      "episode reward mean: 17 0.9995\n",
      "episode reward mean: 18 1.0\n",
      "episode reward mean: 19 1.0\n",
      "episode reward mean: 20 1.0\n",
      "episode reward mean: 21 0.9995\n",
      "episode reward mean: 22 1.0\n",
      "episode reward mean: 23 1.0\n",
      "episode reward mean: 24 1.0\n"
     ]
    }
   ],
   "source": [
    "mean_ppo = []\n",
    "for _ in range(25):\n",
    "    result = algo.train()\n",
    "    print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "    mean_ppo.append(result['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d1bafb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = [x for x in range(len(mean_ppo))]\n",
    "\n",
    "plt.plot(xs, mean_ppo)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63087033",
   "metadata": {},
   "source": [
    "### How to use the trained algorithm in RL with PP0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba1565fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer=algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5aa82a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = trainer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e56c0dc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rusla/ray_results\\PPO_MyEnv_2023-02-03_23-03-12nf8cf6h5\\checkpoint_000025\n"
     ]
    }
   ],
   "source": [
    "print(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc021f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Fix the windows path\n",
    "#evaluation = trainer.evaluate(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98a91c8b",
   "metadata": {},
   "source": [
    "## Computing actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31d32737",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = MyEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "36f4d577",
   "metadata": {},
   "outputs": [],
   "source": [
    "done = False\n",
    "total_reward = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4784046a",
   "metadata": {},
   "outputs": [],
   "source": [
    "observations = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1207b318",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(observations) # The state which you should determine the action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c0f9a8",
   "metadata": {},
   "source": [
    "Given any state compute the action which you get the maximum reward in according to the traning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8e8014b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "action = trainer.compute_single_action(observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "843a04b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fa1bade5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observations, reward, done, info 1 1 True {}\n"
     ]
    }
   ],
   "source": [
    "while not done:\n",
    "    action = trainer.compute_single_action(observations)\n",
    "    observations, reward, done, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    print(\"observations, reward, done, info\",observations, reward, done, info)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "805eca4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'obs_1': 1, 'obs_2': 0}\n"
     ]
    }
   ],
   "source": [
    "action = trainer.compute_actions({\"obs_1\": observations, \"obs_2\": observations})\n",
    "print(action)\n",
    "# {'obs_1': 0, 'ob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630100cd",
   "metadata": {},
   "source": [
    "In the following rl test we are going to use  stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e77ffc67",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'stable_baselines3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m PPO\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmonitor\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Monitor\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstable_baselines3\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvec_env\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DummyVecEnv\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pygame\n",
    "from pygame.surfarray import array3d\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56df269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "env = MyEnv()\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd5eb22d",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = f\"models/{int(time.time())}/\"\n",
    "logdir = f\"logs/{int(time.time())}/\"\n",
    "\n",
    "fps_controller = pygame.time.Clock()\n",
    "fps_controller.tick(60)\n",
    "\n",
    "# Checks for errors encountered\n",
    "pygame.init()\n",
    "\n",
    "# Initialise game window\n",
    "pygame.display.set_caption('Training')\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)\n",
    "\n",
    "\n",
    "env = DummyVecEnv([lambda: Monitor(MyEnv(),logdir,allow_early_resets=True)])\n",
    "\n",
    "model = PPO(\"MlpPolicy\", env,verbose=1, tensorboard_log=logdir,n_epochs=10)\n",
    "\n",
    "TIMESTEPS = 100\n",
    "iters = 0\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*iters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559a5522",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "473615a2",
   "metadata": {},
   "source": [
    "In the following example we are interested to include pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93878ceb",
   "metadata": {},
   "source": [
    "### Example 2 - Gym + Ray + Pygame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e75cc0f",
   "metadata": {},
   "source": [
    "In order to create an appropiate gym enviroment applied to ray and pygame we need need to pay attention into two gym objects:\n",
    "\n",
    "-- action (object): The action to be performed in the step() function. \n",
    "In a game of chess, the action would be the specific, legal move performed by a player.\n",
    "\n",
    "--observation (object): This is all the information available to the agent to choose the next action. \n",
    "The observation is based only on the current state of the environment.\n",
    "\n",
    "## Observation_space and Action_space\n",
    "In  particular  the observation_space and action_space:\n",
    "there are only certain actions and observations are valid in regards of a specific environment. \n",
    "\n",
    "To define a format, the observation_space and action_space variables need to be assigned to a respective gym.space class.\n",
    "\n",
    "Spaces can differ in their dimensionality and their value range. Continuous and discrete spaces are both possible.\n",
    "\n",
    "self.observation_space = <gym.space>\n",
    "self.action_space = <gym.space>\n",
    "\n",
    "\n",
    "We are going to consider an enviroment where there are two\n",
    "points, one red and one blu. The purpose of the game is give a blue point ( worker) where intercept the red point (target)\n",
    "\n",
    "## Definition of action space\n",
    "\n",
    "We want to control the position of the blue point.\n",
    "\n",
    "So the action is the position, the action are the coordinates that you provides to the enviroment\n",
    "\n",
    "action =[x, y]\n",
    "\n",
    "The value of each coordinate are continous and must be in the range of the size of the horizontal box\n",
    "\n",
    "gym.spaces.Box(low=min_x., high=max_x., shape=(2,), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "342a87e7",
   "metadata": {},
   "source": [
    "# Analysis of Spaces\n",
    "Before we continue le us check some examples of spaces in order to understand how it works."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4eb6f",
   "metadata": {},
   "source": [
    "## Box\n",
    "\n",
    "Box - Supports continuous (and discrete) vectors or matrices, used for vector observations, images, etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791065db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.spaces import Box, Discrete,MultiBinary , MultiDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1203f178",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "observation_space = Box(low=-1.0, high=2.0, shape=(3, 4), dtype=np.float32)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301722b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2\n",
    "observation_space = Box(low=np.array([-1.0, -2.0]), high=np.array([2.0, 4.0]), dtype=np.float32)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f3244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 2\n",
    "observation_space = Box(low=np.array(-1.0), high=np.array(2.0), dtype=np.float32)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c906d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1\n",
    "observation_space = Box(low=0, high=200, shape=(2,), dtype=np.float32)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240269e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(observation_space.sample()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f491b366",
   "metadata": {},
   "source": [
    "## Discrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceba308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 3\n",
    "observation_space =Discrete(2)            # {0, 1}\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad09f823",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 4\n",
    "observation_space =Discrete(3)  # {0, 1, 2}\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a723878f",
   "metadata": {},
   "source": [
    "## MultiBinary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04342033",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5\n",
    "observation_space = MultiBinary(5)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd4261c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5\n",
    "observation_space = MultiBinary(2)\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97e85da7",
   "metadata": {},
   "source": [
    "# MultiDiscrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b9a126",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6\n",
    "observation_space =  MultiDiscrete(np.array([[1, 2], [3, 4]]))\n",
    "print(observation_space.sample().shape)\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ade45ca",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc1bff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6a\n",
    "#observation_space =Text(5)\n",
    "# {\"0\", \"42\", \"0123456789\", ...}\n",
    "#observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab5fb298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6b\n",
    "#import string\n",
    "#observation_space = Text(min_length = 1,\n",
    "#     max_length = 10,\n",
    "#     charset = string.digits)\n",
    "#observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fe9905",
   "metadata": {},
   "source": [
    "# Dict\n",
    "Elements of this space are (ordered) dictionaries of elements from the constituent spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c17c139e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 7\n",
    "from gym.spaces import Dict, Discrete\n",
    "observation_space = Dict({\"position\": Discrete(2), \"velocity\": Discrete(3)})\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d125e676",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 8 [nested]:\n",
    "from gym.spaces import Box, Dict, Discrete, MultiBinary, MultiDiscrete\n",
    "observation_space =Dict(\n",
    "    {\n",
    "        \"ext_controller\": MultiDiscrete([5, 2, 2]),\n",
    "        \"inner_state\": Dict(\n",
    "            {\n",
    "                \"charge\": Discrete(100),\n",
    "                \"system_checks\": MultiBinary(10),\n",
    "                \"job_status\": Dict(\n",
    "                    {\n",
    "                        \"task\": Discrete(5),\n",
    "                        \"progress\": Box(low=0, high=100, shape=()),\n",
    "                    }\n",
    "                ),\n",
    "            }\n",
    "        ),\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6235a9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(observation_space.sample())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a327e68f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe9b0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 9\n",
    "from gym.spaces import Box, Discrete\n",
    "observation_space = Dict({\"position\": Box(-1, 1, shape=(2,)), \"color\": Discrete(3)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221e664f",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efa0383",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07829e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=pd.read_csv('seats_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "680e0fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3181278f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#id  x, y z "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba961110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = pd.read_pickle(\"./employees.pkl\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791152fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e486b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00191e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example 10\n",
    "observation_space = = gym.spaces.Dict(\n",
    "    {\"x_position\": gym.spaces.Box(low=0, high=6, shape=(self.max_sit,), dtype=np.uint8),\n",
    "     \"y_position\": gym.spaces.Box(low=0, high=6, shape=(self.max_sit,), dtype=np.uint8),\n",
    "     \"cluster\": gym.spaces.Box(low=0, high=6, shape=(self.max_sit,), dtype=np.uint8),\n",
    "     \"project\": gym.spaces.Box(low=0, high=6, shape=(self.max_sit,), dtype=np.uint8),\n",
    "     \"energy_consumption\": gym.spaces.Box(low=0, high=1, shape=(self.max_sit,)),\n",
    "     \"emp_project\": gym.spaces.Box(low=0, high=6, shape=(1,), dtype=np.int32),\n",
    "     \"emp_energy_consumption\": gym.spaces.Box(low=0.0, high=1.0, shape=(1,), dtype=np.float32)\n",
    "     }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b425077",
   "metadata": {},
   "source": [
    "# Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d539ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 10\n",
    "from gym.spaces import Box, Discrete, Tuple\n",
    "observation_space = Tuple((Discrete(2), Box(-1, 1, shape=(2,))))\n",
    "observation_space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe075c02",
   "metadata": {},
   "source": [
    "# Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dc90c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 11\n",
    "#from gym.spaces import Sequence\n",
    "#space = Sequence(Box(0, 1))\n",
    "#space.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf71d349",
   "metadata": {},
   "source": [
    "for futher information visit\n",
    "https://gymnasium.farama.org/api/spaces/composite/#gymnasium.spaces.Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb061b",
   "metadata": {},
   "source": [
    "# ---- Summary---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4396bf",
   "metadata": {},
   "source": [
    "Discrete - Supports a single discrete number of values with an optional start for the values\n",
    "\n",
    "MultiDiscrete - Supports single or matrices of binary values, used for holding down a button or if an agent has an object\n",
    "\n",
    "MultiBinary - Supports multiple discrete values with multiple axes, used for controller actions\n",
    "\n",
    "Text - Supports strings, used for passing agent messages, mission details, etc\n",
    "\n",
    "Composite Spaces\n",
    "Often environment spaces require joining fundamental spaces together for vectorised environments, separate agents or readability of the space.\n",
    "\n",
    "Dict - Supports a dictionary of keys and subspaces, used for a fixed number of unordered spaces\n",
    "\n",
    "Tuple - Supports a tuple of subspaces, used for multiple for a fixed number of ordered spaces\n",
    "\n",
    "Sequence - Supports a variable number of instances of a single subspace, used for entities spaces or selecting a variable number of actions\n",
    "\n",
    "Graph - Supports graph based actions or observations with discrete or continuous nodes and edge values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9710aae",
   "metadata": {},
   "source": [
    "\n",
    "Example: if we want to build an observation of a PNG image , you can use the follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceec3b55",
   "metadata": {},
   "source": [
    "\n",
    "    # The action and observation spaces need to be gym.spaces objects:\n",
    "    self.action_space = Discrete(4)  # up, left, right, down\n",
    "    # Here's an observation space for 200 wide x 100 high RGB image inputs:\n",
    "    self.observation_space = Box(\n",
    "        low=0, high=255, shape=(100, 200, 3), dtype=np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2bc1c9",
   "metadata": {},
   "source": [
    "The standard structure of the ray  enviroment \n",
    "https://docs.ray.io/en/latest/rllib/rllib-env.html\n",
    "should be:\n",
    "\n",
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    def __init__(self, env_config):\n",
    "        self.action_space = <gym.Space>\n",
    "        self.observation_space = <gym.Space>\n",
    "    def reset(self):\n",
    "        return <obs>\n",
    "    def step(self, action):\n",
    "        return <obs>, <reward: float>, <done: bool>, <info: dict>\n",
    "\n",
    "ray.init()\n",
    "algo = ppo.PPO(env=MyEnv, config={\n",
    "    \"env_config\": {},  # config to pass to env class\n",
    "})\n",
    "\n",
    "while True:\n",
    "    print(algo.train())\n",
    "    \n",
    "    \n",
    "    \n",
    "https://docs.ray.io/en/latest/rllib/rllib-algorithms.html?highlight=%20APPOConfig()#appo    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0953a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3793d6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import pygame\n",
    "from pygame import display\n",
    "from pygame.surfarray import array3d\n",
    "import random\n",
    "\n",
    "BLACK = pygame.Color(0, 0, 0)\n",
    "WHITE = pygame.Color(255, 255, 255)\n",
    "RED = pygame.Color(255, 0, 0)\n",
    "GREEN = pygame.Color(0, 255, 0)\n",
    "BLUE = pygame.Color(0, 0, 255)\n",
    "worker_pos=[0,0]\n",
    "\n",
    "\n",
    "class MyEnv(gym.Env):\n",
    "    \n",
    "    metadata = {'render.modes': ['human']}\n",
    "\n",
    "    def __init__(self, env_config):\n",
    "\n",
    "        # We inizialize the display\n",
    "        self.frame_size_x = 200\n",
    "        self.frame_size_y = 200\n",
    "        self.game_window = pygame.display.set_mode((self.frame_size_x, self.frame_size_y))   \n",
    "        \n",
    "        # Define action and observation space\n",
    "        # They must be gym.spaces objects    \n",
    "            \n",
    "        # ------------------ACTION SPACE----------------------------------\n",
    "        # The action space are the possible actions that is allowed by the worker\n",
    "        # previously was a simple number \n",
    "        # self.action_space = spaces.Discrete(1)\n",
    "        # Now in this example a simple action is a single coordinates (x,y)      \n",
    "        self.action_space = gym.spaces.Box(low=0, high=200, shape=(2,), dtype=np.int32)#dtype=np.float32) \n",
    "        \n",
    "        \n",
    "        # ------------------OBSERVATION SPACE-------------------------------\n",
    "        # Is what the state  is observed\n",
    "        #Previously was a single number discrete\n",
    "        #self.observation_space = gym.spaces.Discrete(2)\n",
    "        # For this example we choose a numpy array   shape=(2,) or an image\n",
    "        # Attention!!! The observation space should have the same shape that the reset \n",
    "        # In order to perform the RL training otherwise will fail!!\n",
    "       \n",
    "        self.observation_space = spaces.Box(low=0, high=200,\n",
    "                                            shape=(2,), dtype=np.int32) #np.float32) \n",
    "        '''\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\"x_position\": gym.spaces.Box(low=0, high=6, shape=(1,), dtype=np.uint8),\n",
    "             \"y_position\": gym.spaces.Box(low=0, high=6, shape=(1,), dtype=np.uint8),\n",
    "             }\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        # Initial conditions\n",
    "        \n",
    "        self.game_window.fill(WHITE)\n",
    "        # Moreover we add a position in the screen display\n",
    "        \n",
    "        # Initial worker position\n",
    "        self.worker_pos=[0,0]\n",
    "        \n",
    "    \n",
    "        #We assing a kind of gym object to a circle \n",
    "        self.worker_rect=pygame.draw.circle(self.game_window,BLUE,(self.worker_pos[0], self.worker_pos[1]),6) # DRAW CIRCLE\n",
    "\n",
    "        # Initial target position\n",
    "        \n",
    "        self.target_pos = [100, 100]\n",
    "        print('Initial target position',self.target_pos[0],self.target_pos[1])\n",
    "        self.target_rect=pygame.draw.circle(self.game_window,RED,(self.target_pos[0], self.target_pos[1]),6) # DRAW CIRCLE\n",
    "        self.steps = 0\n",
    "\n",
    "    def reward_value(self,worker,target):\n",
    "         #Check for collision between two rects\n",
    "        if worker.colliderect(target):\n",
    "            #print(\"worker, target\",worker, target)\n",
    "            reward=1.0\n",
    "            done=True    \n",
    "        else:\n",
    "            reward=-1.0\n",
    "            done=False\n",
    "        return reward\n",
    "              \n",
    "    def step(self, action):\n",
    "        #reward = 0.0\n",
    "        \n",
    "        self.worker_pos = action\n",
    "        #print('worker_pos:'self.worker_pos)\n",
    "            \n",
    "        # We update the state with an image ( in other words plot the points due to the action)\n",
    "        self.update_game_state()\n",
    "\n",
    "        #print(self.worker_rect,self.target_rect)\n",
    "        \n",
    "        reward= self.reward_value(self.worker_rect,self.target_rect)\n",
    " \n",
    "        # regardless of the action, game is done after step becomes true\n",
    "        reward_tmp, done = self.game_over(reward)\n",
    "        \n",
    "        #self.reward=self.reward+reward_tmp\n",
    "        self.reward=reward_tmp\n",
    "        \n",
    "        \n",
    "        info = {}        \n",
    "        \n",
    "        \n",
    "        #Accumulative reward\n",
    "        print('Reward in step:',self.steps,self.reward)\n",
    "        \n",
    "        # -----Under the assuption that we deal with an image---\n",
    "        #img = self.get_image_array_from_game()\n",
    "        #state=img\n",
    "        #--------------------------------------------------------\n",
    "        self.state=[self.target_pos[0], self.target_pos[1]]\n",
    "\n",
    "        #print('step:', self.steps)\n",
    "        self.steps += 1\n",
    "        \n",
    "        observation = np.array(self.state, dtype=np.int32) #float32)\n",
    "        return observation, self.reward, done, info\n",
    "    \n",
    "    def worker_step(self,event):   \n",
    "        '''\n",
    "        Takes human keyboard event and then returns it as an action string\n",
    "        '''\n",
    "        action = None\n",
    "        if event.type == pygame.QUIT:\n",
    "            pygame.quit()\n",
    "            sys.exit()\n",
    "            \n",
    "        #Move based on mouse clicks\n",
    "        if event.type == pygame.MOUSEBUTTONDOWN:\n",
    "            #print(event)\n",
    "            mouse_x = event.pos[0]\n",
    "            mouse_y = event.pos[1]\n",
    "            self.worker_pos[0]=mouse_x\n",
    "            self.worker_pos[1]=mouse_y\n",
    "            action = [self.worker_pos[0], self.worker_pos[1]]\n",
    "        #Drag the object when the mouse button is clicked\n",
    "        \n",
    "        if event.type == pygame.MOUSEMOTION and event.buttons[0] == 1:\n",
    "            #print(event)\n",
    "            mouse_x = event.pos[0]\n",
    "            mouse_y = event.pos[1]\n",
    "            self.worker_pos[0]=mouse_x\n",
    "            self.worker_pos[1]=mouse_y\n",
    "            action = [self.worker_pos[0], self.worker_pos[1]]   \n",
    "        \n",
    "        elif event.type == pygame.KEYDOWN:\n",
    "            # Esc -> Create event to quit the game\n",
    "            if event.key == pygame.K_ESCAPE:\n",
    "                pygame.event.post(pygame.event.Event(pygame.QUIT))                \n",
    "        \n",
    "        return  action    \n",
    "    \n",
    "    def update_game_state(self):\n",
    "        \n",
    "        #We fill the screen to white\n",
    "        self.game_window.fill(WHITE)\n",
    "        \n",
    "        #Draw rectangles to represent the rect's of each object\n",
    "        # For the worker\n",
    "        \n",
    "        self.worker_rect.x=self.worker_pos[0]\n",
    "        self.worker_rect.y=self.worker_pos[1]\n",
    "        self.worker_rect=pygame.draw.circle(self.game_window,BLUE,(self.worker_rect.x,self.worker_rect.y),6) # DRAW CIRCLE\n",
    "        \n",
    "        # For the target\n",
    "        self.target_rect.x=self.target_pos[0]\n",
    "        self.target_rect.y=self.target_pos[1]        \n",
    "        pygame.draw.circle(self.game_window,RED,(self.target_rect.x,self.target_rect.y),6) # DRAW CIRCLE\n",
    "\n",
    "\n",
    "    def get_image_array_from_game(self):\n",
    "        img = array3d(display.get_surface())\n",
    "        #Preprocessing of channels ( needed for tensorflow)\n",
    "        img = np.swapaxes(img, 0, 1)\n",
    "        return img    \n",
    "    \n",
    "    def reset(self):\n",
    "        \n",
    "        # Type 1 Observation\n",
    "        #print('Target position',self.target_pos[0], self.target_pos[1])\n",
    "        #observation = np.array([self.target_pos[0], self.target_pos[1]])\n",
    "        \n",
    "        #Type 2 Observation\n",
    "        #We create a simple observation state from a sample\n",
    "        #self.state = self.observation_space.sample()\n",
    "        #observation = np.array(self.state, dtype=np.int32) #float32)\n",
    "\n",
    "        #Type 2 From a random sit\n",
    "        #sit_random = np.random.randint(1, 9)\n",
    "        #self.state['x_position'][sit_random:] = 0\n",
    "        #self.state['y_position'][sit_random:] = 0\n",
    "        #observation = self.state\n",
    "        \n",
    "        #Type 4 From a target\n",
    "        self.state=[self.target_pos[0], self.target_pos[1]]        \n",
    "        observation = np.array(self.state, dtype=np.int32) #float32)       \n",
    "        \n",
    "        # Type 3 Observation\n",
    "        #img = array3d(display.get_surface())\n",
    "        #img = np.swapaxes(img, 0, 1)\n",
    "        #observation=img\n",
    "        \n",
    "        self.steps = 0\n",
    "        self.reward = 0\n",
    "\n",
    "        #print(\"Game Reset.\")\n",
    "        #print('observation',observation)\n",
    "        return observation    \n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        if mode == \"human\":\n",
    "            display.update()        \n",
    "    def close(self):\n",
    "        pass\n",
    "    \n",
    "    def game_over(self, reward):\n",
    "        if (reward < 1) or (self.steps >= 1000): \n",
    "            return -1, False\n",
    "        else:\n",
    "            return reward, True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0adc5cb",
   "metadata": {},
   "source": [
    "# Ray testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c90445e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "ray.init()\n",
    "algo = ppo.PPO(env=MyEnv, config={\n",
    "    \"env_config\": {},  # config to pass to env class\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173226d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b280551f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ppo = []\n",
    "for _ in range(25):\n",
    "    result = algo.train()\n",
    "    print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "    mean_ppo.append(result['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da573600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "xs = [x for x in range(len(mean_ppo))]\n",
    "\n",
    "plt.plot(xs, mean_ppo)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47a1a539",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827ef044",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0448d61",
   "metadata": {},
   "source": [
    "# Testing Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca5cca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method 1 - use local test class\n",
    "# Testing local frame\n",
    "env = MyEnv(env_config={})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aa2b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test=env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd66c4c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb87f3fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "action = env.action_space.sample()\n",
    "print('action',action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb037bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#action=[100,100]\n",
    "state, reward, done, info = env.step(action)\n",
    "print(\"Reward = {} with action = {}\".format(reward,action))\n",
    "import matplotlib.pyplot as plt\n",
    "print(reward, done, info)\n",
    "#state = np.array(state)\n",
    "\n",
    "print(state,type(state))\n",
    "#plt.figure()\n",
    "#plt.imshow(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f727b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing multiple frames\n",
    "import sys\n",
    "env = MyEnv(env_config={})\n",
    "env.reset()\n",
    "# This is technically a FPS Refresh rate\n",
    "FPS = 10\n",
    "# FPS (frames per second) controller\n",
    "fps_controller = pygame.time.Clock()\n",
    "# Checks for errors encountered\n",
    "check_errors = pygame.init()\n",
    "# Initialise game window\n",
    "pygame.display.set_caption('Testing Game') \n",
    "#The main game loop\n",
    "running = True\n",
    "while running:\n",
    "    # Check Input from Human Step \n",
    "    for event in pygame.event.get():\n",
    "        action = env.worker_step(event)    \n",
    "        if event.type == pygame.QUIT:\n",
    "            running = False\n",
    "        pygame.display.update()\n",
    "        if action != None :# (0,0) :# and reward!=0:\n",
    "            #print(action,type(action))\n",
    "            state, reward, done, info = env.step(action)\n",
    "            print(\"Reward = {} with action = {} , done = {}\".format(reward,action,done))\n",
    "            # Refresh game screen    \n",
    "    # Refresh rate\n",
    "    fps_controller.tick(FPS)\n",
    "    img = array3d(env.game_window)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0b4158",
   "metadata": {},
   "source": [
    "# stable_baselines3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ef3bd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_checker import check_env\n",
    "#env = MyEnv()\n",
    "env = MyEnv(env_config={})\n",
    "\n",
    "check_env(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5491284f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import os\n",
    "import time\n",
    "import pygame\n",
    "from pygame.surfarray import array3d\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de15b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = f\"models/{int(time.time())}/\"\n",
    "logdir = f\"logs/{int(time.time())}/\"\n",
    "\n",
    "fps_controller = pygame.time.Clock()\n",
    "fps_controller.tick(60)\n",
    "\n",
    "# Checks for errors encountered\n",
    "pygame.init()\n",
    "\n",
    "# Initialise game window\n",
    "pygame.display.set_caption('Traning')\n",
    "\n",
    "if not os.path.exists(models_dir):\n",
    "    os.makedirs(models_dir)\n",
    "\n",
    "if not os.path.exists(logdir):\n",
    "    os.makedirs(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74262379",
   "metadata": {},
   "outputs": [],
   "source": [
    "envrl = DummyVecEnv([lambda: Monitor(env,logdir,allow_early_resets=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78d47577",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PPO(\"MlpPolicy\", env,verbose=1, tensorboard_log=logdir,n_epochs=40)\n",
    "#model = PPO(\"MultiInputPolicy\", envrl,verbose=1, tensorboard_log=logdir,n_epochs=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f794e329",
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMESTEPS = 100\n",
    "iters = 0\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    iters += 1\n",
    "    model.learn(total_timesteps=TIMESTEPS, reset_num_timesteps=False, tb_log_name=f\"PPO\")\n",
    "    model.save(f\"{models_dir}/{TIMESTEPS*iters}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "247f5a03",
   "metadata": {},
   "source": [
    "### Init Ray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a4241a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray\n",
    "#from ray.rllib.env.env_context import EnvContext\n",
    "#from ray.rllib.algorithms import appo\n",
    "#from ray.rllib.algorithms.appo import APPOConfig\n",
    "if not ray.is_initialized():\n",
    "    ray.init()\n",
    "    assert ray.is_initialized()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c87d3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray.rllib.algorithms.appo import APPOConfig\n",
    "config = (\n",
    "    APPOConfig()\n",
    "    .rollouts(horizon=10000)\n",
    "    .environment(\n",
    "        MyEnv,\n",
    "        env_config={}\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25f1a25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8cefe4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ea54d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aabcfea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb55fa68",
   "metadata": {},
   "source": [
    "# ray.rllib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2074ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym, ray\n",
    "from ray.rllib.algorithms import ppo\n",
    "#import ray.rllib.agents.ppo as ppo\n",
    "from ray.rllib.algorithms.ppo import PPOConfig\n",
    "from ray.tune.logger import pretty_print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc93b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(ray)\n",
    "ray.shutdown()\n",
    "ray.init(ignore_reinit_error=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795ca4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ppo.DEFAULT_CONFIG.copy()\n",
    "config[\"log_level\"] = \"WARN\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a52787",
   "metadata": {},
   "outputs": [],
   "source": [
    "algo = ppo.PPO(env=MyEnv, config=config) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c87d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Dashboard URL: http://{}\".format(ray.get_webui_url()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5729dc1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#config = PPOConfig()#.rollouts(horizon=200) \n",
    "config={\n",
    "        \"env\": MyEnv,\n",
    "        \"num_workers\": 30,\n",
    "        \"num_cpus_per_worker\": 0.5,\n",
    "        \"env_config\":{\n",
    "            \"max_steps\": 1000,\n",
    "            \"export_frames\": False,\n",
    "            \"export_states\": False,\n",
    "            # \"reward_mode\": \"continuous\",\n",
    "            # \"env_flipped\": True,\n",
    "            # \"env_flipmode\": True,\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1277c08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#algo = ppo.PPO(env=MyEnv, config={\"env_config\": {},  # config to pass to env class\n",
    "#})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc89f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_ppo = []\n",
    "for _ in range(25):\n",
    "    result = algo.train()\n",
    "    print(\"episode reward mean:\", _, result['episode_reward_mean'])\n",
    "    mean_ppo.append(result['episode_reward_mean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc7dfcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ray import tune\n",
    "tune.run(\n",
    "    \"SAC\", # reinforced learning agent\n",
    "    name = \"Training2\",\n",
    "    checkpoint_freq = 100,\n",
    "    checkpoint_at_end = True,\n",
    "    local_dir = r'./ray_results/',\n",
    "    config={\n",
    "        \"env\": MyEnv,\n",
    "        \"num_workers\": 30,\n",
    "        \"num_cpus_per_worker\": 0.5,\n",
    "        \"env_config\":{\n",
    "            \"max_steps\": 1000,\n",
    "            \"export_frames\": False,\n",
    "            \"export_states\": False,\n",
    "            # \"reward_mode\": \"continuous\",\n",
    "            # \"env_flipped\": True,\n",
    "            # \"env_flipmode\": True,\n",
    "            }\n",
    "        },\n",
    "    stop = {\n",
    "        \"timesteps_total\": 5_000_000,\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d6472e",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_ITER = 30\n",
    "s = \"{:3d} reward {:6.2f}/{:6.2f}/{:6.2f} len {:6.2f} saved {}\"\n",
    "\n",
    "for n in range(N_ITER):\n",
    "  result = algo.train()\n",
    "  #file_name = agent.save(CHECKPOINT_ROOT)\n",
    "\n",
    "  print(s.format(\n",
    "    n + 1,\n",
    "    result[\"episode_reward_min\"],\n",
    "    result[\"episode_reward_mean\"],\n",
    "    result[\"episode_reward_max\"],\n",
    "    result[\"episode_len_mean\"],\n",
    "    file_name\n",
    "   ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf07855a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Gym)",
   "language": "python",
   "name": "gym"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
